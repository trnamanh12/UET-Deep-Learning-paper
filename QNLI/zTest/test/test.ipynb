{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trnmah/mambaforge/envs/practic1/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from w2v import Word2Vec\n",
    "from data import SquadDataset\n",
    "from rnn import RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nyu-mll/glue\", \"qnli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When did the third Digimon series begin?',\n",
       " 'sentence': 'Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese.',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_train = dataset['train'].select(range(2269,6999))\n",
    "random_val = dataset['validation'].select(range(2269,2869))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SquadDataset(random_train, 128, tokenizer)\n",
    "validation_data = SquadDataset(random_val, 24, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNe(nn.Module):\n",
    "\tdef __init__(self, args):\n",
    "\t\tsuper(RNNe, self).__init__()\n",
    "\t\tself.w2v = Word2Vec(args.vocab_size, args.embed_size)\n",
    "\t\tself.rnn = RNN(args.embed_size, args.hidden_size, args.num_layers)\n",
    "\t\n",
    "\tdef forward(self, c, q):\n",
    "\t\tc = self.w2v(c)\n",
    "\t\tq = self.w2v(q)\n",
    "\t\tout = self.rnn(c, q)\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "\t'vocab_size': tokenizer.vocab_size,\n",
    "\t'embed_size': 100,\n",
    "\t'hidden_size': 128,\n",
    "\t'num_layers': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNe(args).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critereon = nn.CrossEntropyLoss().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, critereon, epochs=10):\n",
    "    t0 = time.time()\n",
    "\tfor i in range(epochs):\n",
    "\t\tmodel.train()\n",
    "\t\trunning_loss = 0\n",
    "\t\tfor s, q, l in train_data:\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\ts = s['input_ids'].to('cuda')\n",
    "\t\t\tq = q['input_ids'].to('cuda')\n",
    "\t\t\tl = l.long().to('cuda')\n",
    "\t\t\twith torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "\t\t\t\toutput = model(s, q)\n",
    "\t\t\t\tloss = critereon(output, l)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\trunning_loss += loss.item()\n",
    "\t\tprint(f'Epoch {i}, Loss: {running_loss/len(train_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train( model, optimizer, critereon, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, critereon):\n",
    "\tmodel.eval()\n",
    "\trunning_loss = 0\n",
    "\tfor s, q, l in validation_data:\n",
    "\t\ts = s['input_ids'].to('cuda')\n",
    "\t\tq = q['input_ids'].to('cuda')\n",
    "\t\tl = l.long().to('cuda')\n",
    "\t\toutput = model(s, q)\n",
    "\t\tloss = critereon(output, l)\n",
    "\t\trunning_loss += loss.item()\n",
    "\tprint(f'Validation Loss: {running_loss/len(validation_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3]) torch.Size([2, 3])\n",
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.],\n",
      "         [7., 8., 9.]],\n",
      "\n",
      "        [[1., 2., 3.],\n",
      "         [4., 5., 6.],\n",
      "         [7., 8., 9.]]])\n",
      "tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[[1,2,3],[4,5,6],[7,8,9]],[[1,2,3],[4,5,6],[7,8,9]]])\n",
    "b = torch.Tensor([[1,1, 0], [1, 0, 0]])\n",
    "print(a.shape, b.shape)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.],\n",
       "         [4., 5., 6.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[1., 2., 3.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.masked_fill(b.unsqueeze(-1) == 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, critereon):\n",
    "\tmodel.eval()\n",
    "\trunning_loss = 0\n",
    "\tfor s, q, l in validation_data:\n",
    "\t\ts = s['input_ids'].to('cuda')\n",
    "\t\tq = q['input_ids'].to('cuda')\n",
    "\t\tl = l.long().to('cuda')\n",
    "\t\toutput = model(s, q)\n",
    "\t\tloss = critereon(output, l)\n",
    "\t\trunning_loss += loss.item()\n",
    "\tprint(f'Validation Loss: {running_loss/len(validation_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1780,  2.3726,  0.1212,  0.9717, -0.5141,  1.8723,  1.8695,\n",
       "           0.3158, -0.9684,  1.0252],\n",
       "         [ 1.5407,  1.9876, -0.4566,  1.4335, -1.0039,  1.9742, -1.0171,\n",
       "           1.0240,  0.0887,  2.3723],\n",
       "         [ 0.0669, -0.0659,  0.4259,  0.8481, -0.1229,  1.9464, -2.0172,\n",
       "           2.7150, -0.4906,  0.4149],\n",
       "         [-0.4627, -3.2527,  2.4779,  1.0126,  0.3104,  1.4989,  1.1695,\n",
       "           2.0811,  0.6647,  1.2333],\n",
       "         [ 0.0197, -3.7601,  0.4670,  0.6169, -0.1290,  1.3264,  1.0500,\n",
       "           0.6898,  0.4134, -0.0589],\n",
       "         [-1.3704, -0.2010,  1.4713,  0.5881, -0.1401,  1.3658, -1.3642,\n",
       "           1.1760, -0.8576, -0.5149],\n",
       "         [-0.7665,  0.5607,  0.7573,  0.3028,  0.3197,  1.3995, -1.3331,\n",
       "          -0.0302,  0.2542,  0.1805],\n",
       "         [ 0.6938,  0.0803,  2.7115,  0.0466,  0.3521,  1.0088,  0.5934,\n",
       "           0.5734, -0.2874,  0.4161],\n",
       "         [ 1.6889,  1.0880, -0.3118,  1.1236,  0.8956,  0.7725,  1.3381,\n",
       "           1.5693,  1.3475,  0.9954]],\n",
       "\n",
       "        [[-1.2069,  2.7540,  0.6202,  2.3741, -2.2482, -0.1328,  0.3412,\n",
       "           0.1281, -0.4676,  2.7790],\n",
       "         [-0.3654,  2.2943,  0.7780,  2.3616, -2.2231, -0.1331,  0.3451,\n",
       "           0.1281, -0.4669,  2.7790],\n",
       "         [ 0.9888,  1.1333,  2.0071, -1.1277, -0.2858,  1.2606, -0.6562,\n",
       "           0.6295,  0.9870,  1.2019],\n",
       "         [-0.3376, -0.3407,  0.4487,  0.2442,  0.4104,  0.8829,  1.0483,\n",
       "           2.4312,  0.2058,  0.3467],\n",
       "         [-0.4207, -0.4061,  0.8070,  2.1631, -0.3506,  1.6645,  0.2822,\n",
       "           2.5134,  0.3058, -0.3634],\n",
       "         [-0.8517, -0.2857,  1.7694,  0.7794,  1.6687, -0.1711, -1.0147,\n",
       "           0.8291,  0.1792,  0.6746],\n",
       "         [-0.8007,  0.8971,  1.3287,  0.9473, -1.6198,  0.7735, -0.3923,\n",
       "           1.3672,  0.1008,  0.8519],\n",
       "         [ 0.9317,  2.9518,  1.7252,  2.3410,  0.0269,  2.0920, -1.3344,\n",
       "           1.6046,  0.0954,  2.5829],\n",
       "         [ 0.5344, -0.8665,  2.1587,  0.7391,  0.3848,  1.6560, -1.2678,\n",
       "           1.1468, -0.2383,  1.8789]],\n",
       "\n",
       "        [[-0.5973,  0.9835,  1.0997,  0.5404, -0.1436,  0.3766, -0.4888,\n",
       "          -0.6838,  2.2352,  1.1996],\n",
       "         [ 0.9487, -0.0291,  1.2152,  1.0647,  1.5685, -0.1635, -1.0307,\n",
       "           0.8293,  0.1766,  0.6746],\n",
       "         [ 0.3673, -1.0756,  1.2819,  0.3294, -0.0808,  2.7089, -0.8168,\n",
       "          -0.0906,  1.0236,  0.6512],\n",
       "         [ 0.8510,  0.9537,  0.1571,  1.7442,  0.9972,  1.5770,  0.8547,\n",
       "           1.3206, -0.6815,  1.3146],\n",
       "         [-2.8903, -2.3689,  1.5967,  0.3261,  0.2596, -0.0652, -0.2840,\n",
       "          -0.8919,  1.7984,  2.0346],\n",
       "         [-1.3105,  1.1521,  0.2605,  1.3278, -1.1836,  1.7453, -0.3629,\n",
       "           1.7893, -0.2398,  1.5171],\n",
       "         [-2.0169,  1.7291, -0.4687,  0.1424, -0.7146,  1.3313, -0.6963,\n",
       "           1.1946,  0.7245,  2.4277],\n",
       "         [ 0.0267,  0.2548, -0.4419, -0.1844, -1.0055,  2.4785,  1.7116,\n",
       "           1.7955, -0.8512,  0.8299],\n",
       "         [ 0.3316, -0.6688,  0.9345,  1.0873,  0.4912,  0.9006,  0.9653,\n",
       "           1.2714, -1.2831,  3.4185]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embed_size) :\n",
    "\t\tsuper(Word2Vec, self).__init__()\n",
    "\t\tself.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.embeddings(x)\n",
    "\t\treturn x\n",
    "\t\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\tdef __init__(self,embed_size, max_len = 96):\n",
    "\t\tsuper(PositionalEmbedding, self).__init__()\n",
    "\t\tself.encoding = torch.zeros(max_len, embed_size, requires_grad=False)\n",
    "\t\tpos = torch.arange(0, max_len).float().\tunsqueeze(1)\n",
    "\t\t_2i = torch.arange(0, embed_size, 2).float()\n",
    "\t\tself.encoding[:, 0::2] = torch.sin(pos/ torch.pow(10000, _2i/ embed_size))\n",
    "\t\tself.encoding[:, 1::2] = torch.cos(pos/ torch.pow(10000, _2i/ embed_size))\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# bs, seqlen, embed_dim = x.size()\n",
    "\t\t# pe_tensor = torch.zeros(seqlen, embed_dim)\n",
    "\t\t# sin = [torch.sin(pos/ torch.pow(10000, torch.arange(0, embed_dim, 2)/ embed_dim)) for  pos in self.pos]\n",
    "\t\t# cos = [torch.cos(pos/ torch.pow(10000, torch.arange(1, embed_dim, 2)/ embed_dim)) for pos in self.pos]\n",
    "\t\t# pe_tensor[:, 0::2] = sin\n",
    "\t\t# pe_tensor[:, 1::2] = cos\n",
    "\t\t# pe_tensor = pe_tensor.unsqueeze(0).expand(bs, seqlen, embed_dim)\n",
    "\t\tbs, seqlen, embed_dim = x.size()\n",
    "\t\treturn self.encoding[:seqlen, :].expand(bs, seqlen, embed_dim)\n",
    "\n",
    "class WordEmbedding(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embed_size, max_len = 96):\n",
    "\t\tsuper(WordEmbedding, self).__init__()\n",
    "\t\tself.word2vec = Word2Vec(vocab_size, embed_size)\n",
    "\t\tself.positional_embedding = PositionalEmbedding( embed_size, max_len)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.word2vec(x)\n",
    "\t\tx = x + self.positional_embedding(x)\n",
    "\t\treturn x\n",
    "\n",
    "x = torch.randint(0, 100, (\t3, 9))\n",
    "a = WordEmbedding(100, 10, 9)\n",
    "print(x.size())\n",
    "a(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trnmah/mambaforge/envs/practic1/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the embeddings\n",
    "word_embeddings = model.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "input_ids = tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)\n",
    "\n",
    "# Convert input ids to tensor\n",
    "input_ids_tensor = torch.tensor([input_ids])\n",
    "\n",
    "# Get embeddings for the input ids\n",
    "with torch.no_grad():\n",
    "\tembeddings = word_embeddings(input_ids_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(tokenizer.vocab_size, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.weight = nn.Parameter(model.embeddings.word_embeddings.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(input_ids_tensor) - embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(input_ids_tensor).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = dict(a = '36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "dict.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practic1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
