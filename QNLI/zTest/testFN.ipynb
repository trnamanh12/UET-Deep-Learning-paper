{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nyu-mll/glue\", \"qnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([101,102,103, 104], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 8667, 117, 102, 1139, 3676, 1110, 10509, 102]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello, [SEP] my dog is cute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mset\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/envs/practic1/lib/python3.10/site-packages/torch/utils/data/dataset.py:209\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/practic1/lib/python3.10/site-packages/torch/utils/data/dataset.py:209\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "set(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'sentence', 'label', 'idx'],\n",
       "        num_rows: 104743\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'sentence', 'label', 'idx'],\n",
       "        num_rows: 5463\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'sentence', 'label', 'idx'],\n",
       "        num_rows: 5463\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_train = dataset['train'].select(range(2269,12269))\n",
    "random_val = dataset['validation'].select(range(2269,3269))\n",
    "random_test = dataset['validation'].select(range(3269, 4269))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5013"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(random_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Which artist guested on a live version of Queen's The Show Must Go On?\",\n",
       " 'marshall jefferson got involved in house music after hearing whose music?',\n",
       " \"What trend led to the decrease of Estonia's GDP?\",\n",
       " 'Which architects in the US and Britain still employ the Georgian style for private residences?',\n",
       " 'Who was responsible for crafting a new look for all Apple products?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_train['question'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataset is a list of dicts with 'question', 'sentence', and 'label' keys\n",
    "def template(template_type, dataset):\n",
    "\tif template_type == 'normal':\n",
    "\t\tinputs= tokenizer([f\"{q['question']}  [SEP] {q['sentence']}\" for q in dataset],\n",
    "\t\t\t\t\t\tpadding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "\telif template_type == 'PCP':\n",
    "\t\tinputs= tokenizer([f\"{q['question']}  [MASK] , {q['context']}\" for q in dataset], \n",
    "\t\t\t\t\tpadding=True, truncation=True, return_tensors=\"pt\", max_length=256)\t\t\n",
    "\ttrain_labels = torch.tensor([q['label'] for q in dataset])\n",
    "\treturn inputs, train_labels\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_outputs = template('normal',random_train)\n",
    "valid_inputs, valid_outputs = template('normal',random_val)\n",
    "test_inputs, test_outputs = template('normal',random_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs['input_ids'],train_inputs['token_type_ids'],\n",
    "                            train_inputs['attention_mask'], train_outputs)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "valid_data= TensorDataset(valid_inputs['input_ids'],valid_inputs['token_type_ids'],\n",
    "\t\t\t\t\t\t\tvalid_inputs['attention_mask'], valid_outputs)\n",
    "valid_loader = DataLoader(valid_data, batch_size=32 )\n",
    "\n",
    "test_data = TensorDataset(test_inputs['input_ids'],test_inputs['token_type_ids'],\n",
    "\t\t\t\t\t\t\ttest_inputs['attention_mask'], test_outputs)\n",
    "test_loader = DataLoader(test_data, batch_size=32 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Freeze word_embeddings\n",
    "def setting_model(model):\n",
    "\tfor param in model.bert.parameters():\n",
    "\t\tparam.requires_grad = False\n",
    "\n",
    "\tfor param in model.classifier.parameters():\n",
    "\t\tparam.requires_grad = True \n",
    "\n",
    "\tfor param in model.bert.embeddings.word_embeddings.parameters():\n",
    "\t\tparam.requires_grad = True \n",
    "\n",
    "# # Freeze position_embeddings\n",
    "# for param in model.bert.embeddings.position_embeddings.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Freeze token_type_embeddings\n",
    "# for param in model.bert.embeddings.token_type_embeddings.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting model\n",
    "setting_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "\tpred_flat = np.argmax(preds, axis=1).flatten()\n",
    "\tlabels_flat = labels.flatten()\n",
    "\treturn np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add_adapter(\"qnli\", adapter_type=AdapterType.text_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add_adapter(\"qnli\", adapter_type=AdapterType.text_task, config=\"pfeiffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_warmup_steps=0,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_training_steps=total_steps)\n",
    "\n",
    "# Seed setting for reproducibility\n",
    "seed_val = 42\n",
    "random.seed(seed_val+222)\n",
    "np.random.seed(seed_val+222)\n",
    "torch.manual_seed(seed_val+222)\n",
    "torch.cuda.manual_seed_all(seed_val+222)\n",
    "\n",
    "\n",
    "# Function for formatting elapsed times\n",
    "def format_time(elapsed):\n",
    "\telapsed_rounded = int(round((elapsed)))\n",
    "\treturn str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, loader, optimizer, scheduler, epochs, device):\n",
    "\t# # Set the seed value all over the place to make this reproducible.\n",
    "\t# seed_val = 42\n",
    "\t# random.seed(seed_val)\n",
    "\t# np.random.seed(seed_val)\n",
    "\t# torch.manual_seed(seed_val)\n",
    "\t# torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\t# Store the average loss after each epoch so we can plot them.\n",
    "\tloss_values = []\n",
    "\tfor epoch_i in range(0, epochs):\n",
    "\t\t# Perform one full pass over the training set.\n",
    "\t\tprint('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "\t\t# Measure how long the training epoch takes.\n",
    "\t\tt0 = time.time()\n",
    "\t\t\n",
    "\t\t# Reset the total loss for this epoch.\n",
    "\t\ttotal_loss = 0\n",
    "\t\t\n",
    "\t\t# Put the model into training mode.\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\tfor step, batch in enumerate(loader):\n",
    "\t\t\t# Progress update every 40 batches.\n",
    "\t\t\tif step % 40 == 0 and not step == 0:\n",
    "\t\t\t\tprint('  Batch {:>1,}  of  {:>1,}.    Elapsed: , Loss {:}'.format(step, len(loader) , total_loss / (step+1)))\n",
    "\t\t\t# `batch` contains three pytorch tensors:\n",
    "\t\t\t#   [0]: input ids \n",
    "\t\t\t# \t[1]: token_type_ids\n",
    "\t\t\t#   [2]: attention masks\n",
    "\t\t\t#   [3]: labels \n",
    "\t\t\tb_input_ids = batch[0].to(device)\n",
    "\t\t\tb_token_type_ids = batch[1].to(device)\n",
    "\t\t\tb_input_mask = batch[2].to(device)\n",
    "\t\t\tb_labels = batch[3].to(device)\n",
    "\t\t\t\n",
    "\t\t\t# Always clear any previously calculated gradients before performing a backward pass.\n",
    "\t\t\tmodel.zero_grad()        \n",
    "\t\t\t\n",
    "\t\t\t# Perform a forward pass (evaluate the model on this training batch).\n",
    "\t\t\t# This will return the loss (rather than the model output) because we have provided the `labels`.\n",
    "\t\t\toutputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\n",
    "\t\t\tloss.backward()\n",
    "\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tscheduler.step()\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\tavg_train_loss = total_loss / len(loader)\n",
    "\t\tprint(f\"Average training loss: {avg_train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, loader, device):\n",
    "\tmodel.eval()\n",
    "\ttotal_eval_accuracy = 0\n",
    "\tfor batch in loader:\n",
    "\t\tb_input_ids, b_input_mask, b_labels = batch\n",
    "\t\tb_input_ids = b_input_ids.to(device)\n",
    "\t\tb_input_mask = b_input_mask.to(device)\n",
    "\t\tb_labels = b_labels.to(device)\n",
    "\t\t\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\t\t\n",
    "\t\tlogits = outputs.logits\n",
    "\t\tlogits = logits.detach().cpu().numpy()\n",
    "\t\tlabel_ids = b_labels.to('cpu').numpy()\n",
    "\t\t\n",
    "\t\t# Calculate the accuracy for this batch of test sentences\n",
    "\t\ttotal_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "\t# Report the final accuracy for this validation run\n",
    "\tavg_val_accuracy = total_eval_accuracy / len(loader)\n",
    "\tprint(\"Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "\treturn avg_val_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), 'bert_finetuned_qnli.bin')\n",
    "\n",
    "# Optionally, save the entire model (not recommended due to potential issues when loading)\n",
    "# torch.save(model, 'bert_finetuned_qnli_full_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model architecture as before\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Load the model's state dictionary\n",
    "model.load_state_dict(torch.load('bert_finetuned_qnli.bin'))\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  3  of   4.    Elapsed: 30.\n"
     ]
    }
   ],
   "source": [
    "print('  Batch {:>2,}  of  {:>2,}.    Elapsed: {:}.'.format(3, 4 ,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load the pre-trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 2. Prepare your dataset\n",
    "dataset = load_dataset('your_dataset')  # replace 'your_dataset' with your actual dataset\n",
    "encoded_dataset = dataset.map(lambda examples: tokenizer(examples['text'], truncation=True, padding='max_length'), batched=True)\n",
    "\n",
    "# 3. Define the training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "# 4. Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=encoded_dataset['train'],         # training dataset\n",
    "    eval_dataset=encoded_dataset['validation'],             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load the pre-trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 2. Prepare your dataset\n",
    "def encode_examples(example):\n",
    "    # Encode the question and sentence with truncation and padding\n",
    "    encoding = tokenizer(example['question'], example['sentence'], truncation=True, padding='max_length', max_length=512)\n",
    "    encoding['labels'] = example['label']\n",
    "    return encoding\n",
    "\n",
    "# Assuming dataset is your dataset\n",
    "encoded_dataset = dataset.map(encode_examples, batched=True)\n",
    "\n",
    "# 3. Define the training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "# 4. Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=encoded_dataset['train'],         # training dataset\n",
    "    eval_dataset=encoded_dataset['validation'],             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    labels = inputs.clone()\n",
    "    # We sample a few tokens in each sequence for MLM training (with probability `mlm_probability`)\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # Replace 80% of masked tokens with [MASK]\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # Replace 10% of masked tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # The rest 10% of masked tokens are kept as their original token\n",
    "    return inputs, labels\n",
    "\n",
    "inputs, labels = mask_tokens(encodings[\"input_ids\"], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practic1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
