{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:47:52.290397Z","iopub.status.busy":"2024-06-08T06:47:52.289694Z","iopub.status.idle":"2024-06-08T06:47:55.919362Z","shell.execute_reply":"2024-06-08T06:47:55.918541Z","shell.execute_reply.started":"2024-06-08T06:47:52.290364Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/trnmah/mambaforge/envs/practic1/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# from data import SquadDataset\n","# from model import * \n","# from w2v import Word2Vec\n","from transformers import BertTokenizerFast, BertModel\n","from datasets import load_dataset\n","import math\n","import torch.optim as optim\n","import time"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import BertTokenizer\n","\n","class SquadDataset(torch.utils.data.Dataset):\n","\t'''\n","\t- Creates batches dynamically by padding to the length of largest example\n","\t  in a given batch.\n","\t- Calulates character vectors for contexts and question.\n","\t- Returns tensors for training.\n","\t'''\n","\t\n","\tdef __init__(self, data, batch_size, tokenizer):\n","\t\t\n","\t\tself.batch_size = batch_size\n","\t\tdata = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n","\t\tself.data = data\n","\t\t# self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\t\tself.tokenizer = tokenizer\n","\t\t\n","\t\t\n","\tdef __len__(self):\n","\t\treturn len(self.data)\n","\t\n","\tdef __iter__(self):\n","\t\t'''\n","\t\tCreates batches of data and yields them.\n","\t\t\n","\t\tEach yield comprises of:\n","\t\t:padded_context: padded tensor of contexts for each batch \n","\t\t:padded_question: padded tensor of questions for each batch \n","\t\t:label: \n","\t\t\n","\t\t'''\n","\t\t\n","\t\tfor batch in self.data:\n","\t\t\tquestions = self.tokenizer(batch['question'], max_length = 96, padding='max_length', truncation=True, return_tensors='pt')\n","\t\t\tcontexts = self.tokenizer(batch['sentence'], max_length = 96, padding='max_length', truncation=True, return_tensors='pt')\n","\t\t\tlabels = torch.IntTensor(batch['label']).to(torch.int8)\n","\t\t\t# question, context include input_ids, attention_mask, token_type_ids\n","\t\t\tyield questions['input_ids'], contexts['input_ids'], labels\n","\t\t\t\n","\t\t\t"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:21.532771Z","iopub.status.busy":"2024-06-08T06:49:21.531971Z","iopub.status.idle":"2024-06-08T06:49:21.544939Z","shell.execute_reply":"2024-06-08T06:49:21.544039Z","shell.execute_reply.started":"2024-06-08T06:49:21.532736Z"},"trusted":true},"outputs":[],"source":["class Word2Vec(nn.Module):\n","\tdef __init__(self, vocab_size, embed_size, BERT = False): \n","\t\tsuper(Word2Vec, self).__init__()\n","\t\tif BERT:\n","\t\t\tmodel = BertModel.from_pretrained('bert-base-uncased')\n","\t\t\tself.embeddings = model.embeddings.word_embeddings\n","\t\t\tself.embeddings.requires_grad_(False)\n","\t\telse:\t\n","\t\t\tself.embeddings = nn.Embedding(vocab_size, embed_size)\n","\t\t\ttorch.nn.init.xavier_uniform_(self.embeddings.weight)\n","\tdef forward(self, x):\n","\t\tx = self.embeddings(x)\n","\t\treturn x\n","\t\n","\n","\n","class PositionalEmbedding(nn.Module):\n","\tdef __init__(self,embed_size, max_len, device):\n","\t\tsuper(PositionalEmbedding, self).__init__()\n","\t\tself.encoding = torch.zeros(max_len, embed_size, requires_grad=False, device=device)\n","\t\tpos = torch.arange(0, max_len, device=device).float().unsqueeze(1)\n","\t\t_2i = torch.arange(0, embed_size, 2, device=device).float()\n","\t\tself.encoding[:, 0::2] = torch.sin(pos/ torch.pow(10000, _2i/ embed_size)).to(device)\n","\t\tself.encoding[:, 1::2] = torch.cos(pos/ torch.pow(10000, _2i/ embed_size)).to(device)\n","\n","\tdef forward(self, x):\n","\t\t# bs, seqlen, embed_dim = x.size()\n","\t\t# pe_tensor = torch.zeros(seqlen, embed_dim)\n","\t\t# sin = [torch.sin(pos/ torch.pow(10000, torch.arange(0, embed_dim, 2)/ embed_dim)) for  pos in self.pos]\n","\t\t# cos = [torch.cos(pos/ torch.pow(10000, torch.arange(1, embed_dim, 2)/ embed_dim)) for pos in self.pos]\n","\t\t# pe_tensor[:, 0::2] = sin\n","\t\t# pe_tensor[:, 1::2] = cos\n","\t\t# pe_tensor = pe_tensor.unsqueeze(0).expand(bs, seqlen, embed_dim)\n","\t\tbs, seqlen, embed_dim = x.size()\n","\t\treturn self.encoding[:seqlen, :].expand(bs, seqlen, embed_dim)\n","\n","class WordEmbedding(nn.Module):\n","\tdef __init__(self, vocab_size, embed_size, max_len, device, BERT=False):\n","\t\tsuper(WordEmbedding, self).__init__()\n","\t\tself.word2vec = Word2Vec(vocab_size, embed_size, BERT)\n","\t\tself.positional_embedding = PositionalEmbedding( embed_size, max_len, device)\n","\t\n","\tdef forward(self, x):\n","\t\tx = self.word2vec(x)\n","\t\tx = x + self.positional_embedding(x)\n","\t\treturn x"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:21.948529Z","iopub.status.busy":"2024-06-08T06:49:21.947828Z","iopub.status.idle":"2024-06-08T06:49:22.000293Z","shell.execute_reply":"2024-06-08T06:49:21.999221Z","shell.execute_reply.started":"2024-06-08T06:49:21.948495Z"},"trusted":true},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    def __init__(self, ndim, bias: bool = True, eps: float = 1e-5):\n","        super(LayerNorm, self).__init__()\n","        self.gamma = nn.Parameter(torch.ones(ndim))\n","        self.beta = nn.Parameter(torch.zeros(ndim))\n","        self.eps = eps\n","    \n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, nhead, dropout):\n","        super(SelfAttention, self).__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        self.p_qkv = nn.Linear(embed_size, embed_size*3)\n","        torch.nn.init.xavier_uniform_(self.p_qkv.weight)\n","\n","        self.p_proj = nn.Linear(embed_size, embed_size)\n","        torch.nn.init.xavier_uniform_(self.p_proj.weight)\n","\n","        self.nhead = nhead\n","    \n","    def forward(self, x , attmask):   \n","        '''\n","        1. input q, k, v, attention mask x [bs, nhead, seq_len, embed_size], attmask [bs, seqlen]\n","        2. calculate the attention score\n","        3. add & norm ( dropout residual connection before add )\n","        4. feed forward network\n","        5. add & norm ( dropout residual connection before add )\n","        ensure that output have shape [bs, seqlen, embed_size*n_head]\n","\n","        '''\n","\n","        x = self.p_qkv(x) # [bs, seq_len, embed_size*3]\n","        q, k, v = torch.chunk(x, 3, dim = -1) # q, k, v [bs, seq_len, embed_size]\n","        bs, sqlen, embed_size = q.size()\n","\n","        q = q.view(bs, sqlen, self.nhead, embed_size//self.nhead).transpose(1, 2)\n","        k = k.view(bs, sqlen, self.nhead, embed_size//self.nhead).transpose(1, 2)\n","        v = v.view(bs, sqlen, self.nhead, embed_size//self.nhead).transpose(1, 2)\n","\n","        # configure mask\n","        attmask = attmask.unsqueeze(1).unsqueeze(2) # [bs, 1, 1, seq_len]\n","\n","\n","        att_score = (q @ k.transpose(-2, -1)) * (1.0/math.sqrt(q.size(-1))) # [bs, nhead, seq_len, seq_len]\n","        att_score = att_score.masked_fill(attmask == 0, -10000)\n","        att_score = F.softmax(att_score, dim = -1)\n","        att_score = self.dropout(att_score) # [bs, nhead, seq_len, seq_len]\n","        y = att_score @ v # [bs, nhead, seqlen, embed_size//nhead]\n","        y = y.transpose(1, 2).contiguous().view(bs, sqlen, embed_size)\n","        \n","        # is y need to be go through a linear layer?\n","        y = self.p_proj(y)\n","        return y\n","    \n","class FFN(nn.Module):\n","    def __init__(self, embed_size):\n","            super().__init__()\n","            self.linear1  = nn.Linear(embed_size, embed_size*4)\n","            torch.nn.init.xavier_uniform_(self.linear1.weight)\n","\n","            self.linear2 = nn.Linear(embed_size*4, embed_size)\n","            torch.nn.init.xavier_uniform_(self.linear2.weight)\n","\n","            self.gelu = nn.GELU()\n","            self.dropout = nn.Dropout(0.1)\n","        \n","    def forward(self, x):\n","            x = self.linear1(x)\n","            x = self.gelu(x)\n","            x = self.dropout(x)\n","            x = self.linear2(x)\n","            x = self.dropout(x)\n","            return x\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, embed_size, nhead, dropout,  bias=True, eps=1e-06):\n","        super().__init__()    \n","        self.selfattn = SelfAttention(embed_size, nhead, dropout)\n","        self.ffn = FFN(embed_size)\n","        self.dropout = nn.Dropout(0.1)\n","        self.norm = LayerNorm(embed_size, bias, eps)\n","\n","    def forward(self, x, mask):\n","        # x  [bs, seqlen, embed_size]\n","        _x = x\n","        x = x +  self.dropout(self.selfattn(x, mask))\n","        x = self.norm(x)\n","        _x = x\n","        x = x + self.dropout(self.ffn(x))\n","        x = self.norm(x)\n","        return x, mask\n","\n","\n","\n","class TransformerEnc(nn.Module):\n","    def __init__(self, embed_size,nhead, num_layers =3, c_len = 96, device='cuda'):\n","        super(TransformerEnc, self).__init__()\n","        '''\n","            1. Encoder question and context \n","            2. CNN to get the local conte\n","        '''\n","        self.qencoder = nn.ModuleList([EncoderLayer(embed_size, nhead, 0.1).to(device) for _ in range(num_layers)])\n","        self.cencoder = nn.ModuleList([EncoderLayer(embed_size, nhead, 0.1).to(device) for _ in range(num_layers)])\n","        \n","        self.Wsim = nn.Linear(embed_size*3, 1)\n","        torch.nn.init.xavier_uniform_(self.Wsim.weight)\n","\n","        self.Wdistil = nn.Linear(embed_size*4, embed_size)\n","        torch.nn.init.xavier_uniform_(self.Wsim.weight)\n","\n","        self.synin4 = [EncoderLayer(embed_size*4, nhead, 0.1 ).to(device) for _ in range(num_layers)]\n","\n","        self.Whead1 = nn.Linear(embed_size*4, 1)\n","        torch.nn.init.xavier_uniform_(self.Whead1.weight)\n","\n","        self.Whead2 = nn.Linear(c_len, 2)\n","        torch.nn.init.xavier_uniform_(self.Whead2.weight)\n","\n","        self.dropout = nn.Dropout(0.1)\n","       \n","    def forward(self, c, q, q_mask, c_mask):\n","        # q_mask = q_mask.unsqueeze(-1)\n","        # c_mask = c_mask.unsqueeze(-1)\n","        # q_mask, c_mask [bs, seqlen]\n","        for layer in self.qencoder:\n","            q, q_mask = layer(q, q_mask)\n","        for layer in self.qencoder:\n","            c, c_mask = layer(c, c_mask)\n","        # c = self.encoder(c, c_mask) # [bs, c_len, embed_size]\n","        # q = self.encoder(q, q_mask) # [bs, q_len, embed_size]\n","        # caculate similarity matrix\n","        bs = c.size(0)\n","        c_len = c.size(1)\n","        q_len = q.size(1)\n","\n","        c_sim = c.unsqueeze(2).expand(-1, -1, q_len, -1) # [bs, c_len, q_len, embed_size]\n","        q_sim = c.unsqueeze(1).expand(-1, c_len, -1, -1) # [bs, c_len, q_len, embed_size] \n","        \n","        cq_sim = torch.mul(c_sim, q_sim) # [bs, c_len, q_len, embed_size]\n","        cqcq = torch.cat([c_sim, q_sim, cq_sim], dim=-1) # [bs, c_len, q_len, 3*embed_size]\n","        S = self.Wsim(cqcq).squeeze(-1) # similarity matrix [bs, c_len, q_len]\n","        \n","        # can meet  error such as the shape of mask can't be broadcastable with the shape of the tensor\n","        # can fix by unsqueeze the mask at dim = -1 of q_mask and c_mask\n","        # q_mask = q_mask.unsqueeze(-1)\n","\n","        S_row = S.masked_fill_(q_mask.unsqueeze(1) == 0, -10000)\n","        S_row = F.softmax(S_row, dim=-1) # [bs, c_len, q_len]\n","        A = torch.bmm(S_row, q) # [bs, c_len, embed_size]\n","\n","        # c_mask = c_mask.unsqueeze(-1)\n","        S_col = S.masked_fill_(c_mask.unsqueeze(2) == 0, -10000)\n","        S_col = F.softmax(S_col, dim = 1) # [bs, c_len, q_len]\n","\n","        B = torch.bmm(torch.bmm(S_col, S_row.transpose(1,2)), c) # [bs, c_len, embed_size]\n","\n","        distil = torch.cat([c, A, torch.mul(c, A), torch.mul(c, B)], dim = -1) # [bs, c_len, 4*embed_size]\n","        # distil = self.Wdistil(distil) # distil information [bs, c_len, embed_size]\n","        for layer in self.synin4:\n","            distil, c_mask = layer(distil, c_mask)\n","        # synin4 = self.synin4(distil) \n","        out1 = self.Whead1(distil)\n","\n","        out1 = out1.squeeze(-1)\n","        out2 = self.Whead2(self.dropout(out1))\n","\n","        return out2\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:22.179011Z","iopub.status.busy":"2024-06-08T06:49:22.178147Z","iopub.status.idle":"2024-06-08T06:49:23.369332Z","shell.execute_reply":"2024-06-08T06:49:23.368240Z","shell.execute_reply.started":"2024-06-08T06:49:22.178975Z"},"trusted":true},"outputs":[],"source":["dataset = load_dataset(\"nyu-mll/glue\", \"qnli\" )\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:23.371976Z","iopub.status.busy":"2024-06-08T06:49:23.371277Z","iopub.status.idle":"2024-06-08T06:49:23.451505Z","shell.execute_reply":"2024-06-08T06:49:23.450706Z","shell.execute_reply.started":"2024-06-08T06:49:23.371935Z"},"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:23.453045Z","iopub.status.busy":"2024-06-08T06:49:23.452694Z","iopub.status.idle":"2024-06-08T06:49:23.459510Z","shell.execute_reply":"2024-06-08T06:49:23.458581Z","shell.execute_reply.started":"2024-06-08T06:49:23.453011Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['question', 'sentence', 'label', 'idx'],\n","        num_rows: 104743\n","    })\n","    validation: Dataset({\n","        features: ['question', 'sentence', 'label', 'idx'],\n","        num_rows: 5463\n","    })\n","    test: Dataset({\n","        features: ['question', 'sentence', 'label', 'idx'],\n","        num_rows: 5463\n","    })\n","})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:23.462273Z","iopub.status.busy":"2024-06-08T06:49:23.461653Z","iopub.status.idle":"2024-06-08T06:49:23.473979Z","shell.execute_reply":"2024-06-08T06:49:23.473245Z","shell.execute_reply.started":"2024-06-08T06:49:23.462238Z"},"trusted":true},"outputs":[],"source":["random_train = dataset['train'].select(range(2269,12269))\n","random_val = dataset['validation'].select(range(2269,3269))\n","random_test = dataset['validation'].select(range(3269, 4269))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_length = 128"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data = SquadDataset(random_train, 32, tokenizer, max_length)\n","validation_data = SquadDataset(random_val, 32, tokenizer, max_length)\n","test_data = SquadDataset(random_test, 32, tokenizer, max_length)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:25.519729Z","iopub.status.busy":"2024-06-08T06:49:25.519353Z","iopub.status.idle":"2024-06-08T06:49:25.526429Z","shell.execute_reply":"2024-06-08T06:49:25.525463Z","shell.execute_reply.started":"2024-06-08T06:49:25.519699Z"},"trusted":true},"outputs":[],"source":["class NAQNLI(nn.Module):\n","\tdef __init__(self, config):\n","\t\tsuper(NAQNLI, self).__init__()\n","\t\tself.w2v = WordEmbedding(config['vocab_size'], config['embed_size'], config['c_len'], config['device'],  config['BERT'])\n","\t\tself.enc = TransformerEnc(config['embed_size'], config['nhead'], config['num_layers'], config['c_len'], config['device'])\n","\t\tself.dropout = nn.Dropout(0.1)\n","\tdef forward(self, c, q, q_mask, c_mask):\n","\t\tq = self.w2v(q)\n","\t\tq = self.drop(q)\n","\t\tc = self.w2v(c)\n","\t\tc - self.drop(c)\n","\t\treturn self.enc(c, q, q_mask, c_mask)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:28.670136Z","iopub.status.busy":"2024-06-08T06:49:28.669756Z","iopub.status.idle":"2024-06-08T06:49:28.678551Z","shell.execute_reply":"2024-06-08T06:49:28.677474Z","shell.execute_reply.started":"2024-06-08T06:49:28.670105Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = 'cpu'\n","device"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:28.949766Z","iopub.status.busy":"2024-06-08T06:49:28.949402Z","iopub.status.idle":"2024-06-08T06:49:28.954383Z","shell.execute_reply":"2024-06-08T06:49:28.953473Z","shell.execute_reply.started":"2024-06-08T06:49:28.949736Z"},"trusted":true},"outputs":[],"source":["BERT = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["config = {\n","    'vocab_size': tokenizer.vocab_size,\n","    'embed_size': 768 if BERT else 256,\n","    'nhead': 12,\n","    'num_layers': 4,\n","    'c_len': 128,\n","    'device': device,\n","    'BERT': BERT\n","}"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:29.240271Z","iopub.status.busy":"2024-06-08T06:49:29.239422Z","iopub.status.idle":"2024-06-08T06:49:33.723816Z","shell.execute_reply":"2024-06-08T06:49:33.722616Z","shell.execute_reply.started":"2024-06-08T06:49:29.240230Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["embedding size must be 768\n"]}],"source":["model = NAQNLI(config).to(device)\n","model = torch.compile(model)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:33.726043Z","iopub.status.busy":"2024-06-08T06:49:33.725679Z","iopub.status.idle":"2024-06-08T06:49:33.732490Z","shell.execute_reply":"2024-06-08T06:49:33.731248Z","shell.execute_reply.started":"2024-06-08T06:49:33.726013Z"},"trusted":true},"outputs":[],"source":["optimizer = optim.Adadelta(model.parameters(), lr=0.5, weight_decay=0.0001)\n","critereon = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:33.734371Z","iopub.status.busy":"2024-06-08T06:49:33.733981Z","iopub.status.idle":"2024-06-08T06:49:33.745154Z","shell.execute_reply":"2024-06-08T06:49:33.744436Z","shell.execute_reply.started":"2024-06-08T06:49:33.734336Z"},"trusted":true},"outputs":[],"source":["def train(model, train_data, optimizer, critereon, epochs):\n","\tt0 = time.time()\n","\tfor epoch in range(epochs):\n","\t\tmodel.train()\n","\t\trunning_loss = 0.0\n","\t\tfor q, c, labels in (train_data):\n","\t\t\tmodel.zero_grad()\n","\t\t\tq_i = q['input_ids'].to(device)\n","\t\t\tc_i = c['input_ids'].to(device)\n","\t\t\tq_mask = q['attention_mask'].to(device)\n","\t\t\tc_mask = c['attention_mask'].to(device)\n","\t\t\t# with torch.autocast(device_type=device, dtype=torch.float16):\n","\t\t\toutput = model(c_i, q_i, q_mask, c_mask)\n","\t\t\tlabels = labels.long().to(device)\n","\t\t\tloss = critereon(output,labels )\n","\t\t\tloss.backward()\n","\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\t\t\toptimizer.step()\n","\t\t\trunning_loss += loss.item()\n","\t\tprint(f\"Epoch {epoch} Loss: {loss.item()/len(train_data)}\")\n","\tt1 = time.time()\t\t\n","\tprint(f\"Training time: {t1-t0}\")\n","\t"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T06:49:33.747314Z","iopub.status.busy":"2024-06-08T06:49:33.746966Z","iopub.status.idle":"2024-06-08T06:49:34.147240Z","shell.execute_reply":"2024-06-08T06:49:34.145739Z","shell.execute_reply.started":"2024-06-08T06:49:33.747260Z"},"trusted":true},"outputs":[],"source":["train(model, train_data, optimizer, critereon, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), '/kaggle/working/model1.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Assuming `model` is an instance of the same architecture you trained earlier\n","model.load_state_dict(torch.load('model_weights.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optimzer = torch.optim.Adam(model.parameters(), lr=0.006,betas=(0.8, 0.999), eps=1e-07, weight_decay=0.0001)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_data)*20, eta_min=0.006*0.1, last_epoch=-1, verbose=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def evaluation(model, validation_data, critereon):\n","\tmodel.eval()\n","\trunning_loss = 0.0\n","\t# with torch.no_grad():\n","\tfor q, c, labels in (validation_data):\n","\t\t\tq_i = q['input_ids'].to(device)\n","\t\t\tc_i = c['input_ids'].to(device)\n","\t\t\tq_mask = q['attention_mask'].to(device)\n","\t\t\tc_mask = c['attention_mask'].to(device)\n","\t\t\twith torch.no_grad():\n","\t\t\t# with torch.autocast(device_type=device, dtype=torch.bfloat16):\n","\t\t\t\toutput = model(c_i, q_i, q_mask, c_mask)\n","\t\t\t\tlabels = labels.long().to(device)\n","\t\t\t\tloss = critereon(output.view(-1, 2),labels.view(-1))\n","\t\t\t\trunning_loss += loss.item()\n","\t\t\t\t_, predicted = torch.max(output, 1, dim=-1, keepdim=True)\n","\t\t\t\ttotal += labels.size(0)\n","\t\t\t\tcorrect += (predicted == labels).sum().item()\n","\tprint(f\"Validation Loss: {running_loss/len(validation_data)}\")\n","\tprint(f\"Accuracy: {correct/total}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["evaluation(model, validation_data, critereon)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict(model, questions, sentences, tokenizer, max_len):\n","\tmodel.eval()\n","\twith torch.no_grad():\n","\tq = tokenizer(questions, max_length = max_len, padding='max_length', truncation=True, return_tensors='pt')\n","\tc = tokenizer(sentences, max_length = max_len, padding='max_length', truncation=True, return_tensors='pt')\n","\tq_i = q['input_ids'].to(device)\n","\tc_i = c['input_ids'].to(device)\n","\tq_mask = q['attention_mask'].to(device)\n","\tc_mask = c['attention_mask'].to(device)\n","\toutput = model(c_i, q_i, q_mask, c_mask)\n","\t_, predicted = torch.max(output, 1, dim=-1, keepdim=True)\n","\treturn predicted"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.1.undefined"}},"nbformat":4,"nbformat_minor":4}
