{"cells":[{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:44:18.026446Z","iopub.status.busy":"2024-06-14T04:44:18.025815Z","iopub.status.idle":"2024-06-14T04:44:24.362892Z","shell.execute_reply":"2024-06-14T04:44:24.362139Z","shell.execute_reply.started":"2024-06-14T04:44:18.026415Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import BertTokenizerFast, BertModel\n","# from embed_layer import Word2Vec, ContextualEmbedding\n","# from e2e import E2E \n","# from data import SquadDataset\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:44:24.365385Z","iopub.status.busy":"2024-06-14T04:44:24.364806Z","iopub.status.idle":"2024-06-14T04:44:24.375949Z","shell.execute_reply":"2024-06-14T04:44:24.374917Z","shell.execute_reply.started":"2024-06-14T04:44:24.365357Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import BertTokenizer\n","\n","class SquadDataset(torch.utils.data.Dataset):\n","\t'''\n","\t- Creates batches dynamically by padding to the length of largest example\n","\t  in a given batch.\n","\t- Calulates character vectors for contexts and question.\n","\t- Returns tensors for training.\n","\t'''\n","\t\n","\tdef __init__(self, data, batch_size, tokenizer, max_len):\n","\t\t\n","\t\tself.batch_size = batch_size\n","\t\tdata = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n","\t\tself.data = data\n","\t\t# self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\t\tself.tokenizer = tokenizer\n","\t\tself.max_len = max_len\n","\t\t\n","\tdef __len__(self):\n","\t\treturn len(self.data)\n","\t\n","\tdef __iter__(self):\n","\t\t'''\n","\t\tCreates batches of data and yields them.\n","\t\t\n","\t\tEach yield comprises of:\n","\t\t:padded_context: padded tensor of contexts for each batch \n","\t\t:padded_question: padded tensor of questions for each batch \n","\t\t:label: \n","\t\t\n","\t\t'''\n","\t\t\n","\t\tfor batch in self.data:\n","\t\t\tquestions = self.tokenizer(batch['question'], max_length = self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n","\t\t\tcontexts = self.tokenizer(batch['sentence'], max_length = self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n","\t\t\tlabels = torch.IntTensor(batch['label']).to(torch.int8)\n","\t\t\t# question, context include input_ids, attention_mask, token_type_ids\n","\t\t\tyield questions['input_ids'], contexts['input_ids'], labels"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:44:24.377453Z","iopub.status.busy":"2024-06-14T04:44:24.377175Z","iopub.status.idle":"2024-06-14T04:44:24.389257Z","shell.execute_reply":"2024-06-14T04:44:24.388245Z","shell.execute_reply.started":"2024-06-14T04:44:24.377428Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import BertModel\n","\n","class Word2Vec(nn.Module):\n","\tdef __init__(self, vocab_size, embed_size, BERT = False): \n","\t\tsuper(Word2Vec, self).__init__()\n","\t\tif BERT:\n","\t\t\tmodel = BertModel.from_pretrained('bert-base-uncased')\n","\t\t\tself.embeddings = model.embeddings.word_embeddings\n","\t\t\tself.embeddings.requires_grad_(False)\n","\t\t\tself.linear = nn.Linear(768, embed_size)\n","\t\t\ttorch.nn.init.normal_(self.linear.weight, mean=0, std=0.02)\n","\t\telse:\t\n","\t\t\tself.embeddings = nn.Embedding(vocab_size, embed_size)\n","\t\t\ttorch.nn.init.normal_(self.embeddings.weight, mean=0, std=0.02)\n","\t\t\tself.linear = nn.Linear(embed_size, embed_size)\n","\t\t\ttorch.nn.init.normal_(self.linear.weight, mean=0, std=0.02)\n","\tdef forward(self, x):\n","\t\tx = self.embeddings(x)\n","\t\tx = self.linear(x)\n","\t\treturn x\n","\n","# class Highway(nn.Module):\n","# \tdef __init__(self, args):\n","# \t\tsuper(Highway, self).__init__()\n","# \t\tself.W_proj = nn.Linear(args.hidden_size*2, args.hidden_size*2)\n","# \t\tself.W_gate = nn.Linear(args.hidden_size, args.hidden_size)\n","\t\n","# \tdef forward(self, x):\n","# \t\tx_proj = F.relu(self.W_proj(x))\n","# \t\tx_gate = F.sigmoid(self.W_gate(x))\n","# \t\tx_highway = x_gate * x_proj + (1 - x_gate) * x\n","# \t\treturn x_highway\n","\n","class ContextualEmbedding(nn.Module):\n","\tdef __init__(self, embed_size, hidden_size):\n","\t\tsuper(ContextualEmbedding, self).__init__()\n","\t\tself.RNN= nn.LSTM(input_size=embed_size,\n","\t\t\t\t\t\t\thidden_size=hidden_size,\n","\t\t\t\t\t\t\tnum_layers=1,\n","\t\t\t\t\t\t\tbidirectional=True,\n","\t\t\t\t\t\t\tbatch_first=True,\n","\t\t\t\t\t\t\tdropout=0.1)\n","\tdef forward(self, x):\n","\t\toutput, _ = self.RNN(x)\n","\t\treturn output\n","\t\t"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:44:24.391965Z","iopub.status.busy":"2024-06-14T04:44:24.391113Z","iopub.status.idle":"2024-06-14T04:44:24.407393Z","shell.execute_reply":"2024-06-14T04:44:24.406529Z","shell.execute_reply.started":"2024-06-14T04:44:24.391939Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class E2E(nn.Module):\n","    def __init__(self, hidden_size, c_len):\n","        super(E2E, self).__init__()\n","        '''\n","        input: [bs, qlen, hidden_size*2], [bs, clen, hidden_size*2]\n","        1. We need to calculate the similarity matrix between the context and the query \n","        2. We need to calculate the attention weights for the context and the query [bs, clen, qlen], and then caculate c2q by multiply [bs, clen, qlen] with [bs, qlen, hidden_size*2] is called c2q\n","        3. We need to calculate the attention weights for the query and the context [bs, qlen, clen], and then caculate q2c by multiply [bs, qlen, clen] with [bs, clen, hidden_size*2] is called q2c\n","        4. Then we concat [context, q2c, context*q2c, ] \n","\n","        '''\n","        self.Ws = nn.Linear(hidden_size*6, 1, )\n","        torch.nn.init.normal_(self.Ws.weight, mean=0, std=0.02)\n","\n","        self.rnn = nn.LSTM(input_size=hidden_size*8, hidden_size=hidden_size*2, num_layers=1, bidirectional=True, batch_first=True, dropout=0.1)\n","        self.dropout = nn.Dropout(0.1)\n","\n","        self.last1 = nn.Linear(hidden_size*4*c_len, hidden_size*2 )\n","        self.last2 = nn.Linear(hidden_size*2, 2)\n","        \n","        torch.nn.init.normal_(self.last1.weight, mean=0, std=0.02)\n","        torch.nn.init.normal_(self.last2.weight, mean=0, std=0.02)\n","    def forward(self, c, q):\n","        '''\n","        c: [bs, clen, hidden_size*2]\n","        q: [bs, qlen, hidden_size*2]\n","\n","        '''\n","        bs = c.size(0)\n","        c_len = c.size(1)\n","        q_len = q.size(1)\n","        hidden_size = c.size(2)\n","\n","        _c = c.unsqueeze(2).expand(-1, -1, q_len, -1)\n","        _q = q.unsqueeze(1).expand(-1, c_len, -1, -1)\n","        cq = torch.mul(_c,_q)\n","        input_s = torch.cat([_c,_q,cq], dim=-1) # [bs, clen, qlen, hidden_size*6]\n","\n","        s = self.Ws(input_s).squeeze(-1) #similarity matrix [bs, clen, qlen] \n","\n","        s1 = F.softmax(s, dim=-1)\n","        c2q = torch.bmm(s1, q) # [bs, clen, hidden_size*2], cco the hieu la ta bieu dien cac word trong context bang to hop attention_score*query\n","        c2q = self.dropout(c2q) \n","        \n","        #q2c\n","        s2 = F.softmax(torch.max(s, dim=-1)[0], dim=-1) # [bs, clen]\n","        s2 = s2.unsqueeze(1).expand(-1, q_len, -1) # [bs, qlen, clen]\n","        q2c = torch.bmm(s2, c) # [bs, qlen, hidden_size*2]\n","        q2c = self.dropout(q2c) \n","\n","        #querry-aware representation \n","        G = torch.cat([c, c2q, torch.mul(c, c2q), torch.mul(c, q2c)], dim=-1)    # [bs, clen, hidden_size*8]\n","        M, _ = self.rnn(G) # [bs, clen, hidden_size*8]\n","        M = self.dropout(M) \n","        M = M.contiguous().view(bs, -1)\n","        out1 = self.last1(M) # [bs, hidden_size*4]\n","\n","        out1 = self.dropout(out1)\n","        out2 = self.last2(F.gelu(out1)) # [bs, 2]\n","        return out2\n","\n"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:44:24.408605Z","iopub.status.busy":"2024-06-14T04:44:24.408355Z","iopub.status.idle":"2024-06-14T04:44:37.402056Z","shell.execute_reply":"2024-06-14T04:44:37.401072Z","shell.execute_reply.started":"2024-06-14T04:44:24.408583Z"},"trusted":true},"outputs":[],"source":["\n","dataset = load_dataset(\"nyu-mll/glue\", \"qnli\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['question', 'sentence', 'label', 'idx'],\n","        num_rows: 104743\n","    })\n","    validation: Dataset({\n","        features: ['question', 'sentence', 'label', 'idx'],\n","        num_rows: 5463\n","    })\n","    test: Dataset({\n","        features: ['question', 'sentence', 'label', 'idx'],\n","        num_rows: 5463\n","    })\n","})"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["3273.21875"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["104743/32"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["['avc', 'def']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["\"avc def\".split(\" \")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:44:37.414299Z","iopub.status.busy":"2024-06-14T04:44:37.413958Z","iopub.status.idle":"2024-06-14T04:44:40.307315Z","shell.execute_reply":"2024-06-14T04:44:40.306331Z","shell.execute_reply.started":"2024-06-14T04:44:37.414269Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/trnmah/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:44:40.309921Z","iopub.status.busy":"2024-06-14T04:44:40.309547Z","iopub.status.idle":"2024-06-14T04:44:40.325485Z","shell.execute_reply":"2024-06-14T04:44:40.324553Z","shell.execute_reply.started":"2024-06-14T04:44:40.309889Z"},"trusted":true},"outputs":[],"source":["random_train = dataset['train'].select(range(2269,12269))\n","random_val = dataset['validation'].select(range(2269,3269))\n","random_test = dataset['validation'].select(range(3269,4269))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T05:25:49.570742Z","iopub.status.busy":"2024-06-14T05:25:49.570122Z","iopub.status.idle":"2024-06-14T05:25:49.667231Z","shell.execute_reply":"2024-06-14T05:25:49.666255Z","shell.execute_reply.started":"2024-06-14T05:25:49.570705Z"},"trusted":true},"outputs":[],"source":["train_data = SquadDataset(random_train, 32, tokenizer, 128)\n","validation_data = SquadDataset(random_val, 32, tokenizer, 128)\n","test_data = SquadDataset(random_test, 16, tokenizer, 128)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:45:05.189314Z","iopub.status.busy":"2024-06-14T04:45:05.188646Z","iopub.status.idle":"2024-06-14T04:45:05.195704Z","shell.execute_reply":"2024-06-14T04:45:05.194681Z","shell.execute_reply.started":"2024-06-14T04:45:05.189282Z"},"trusted":true},"outputs":[{"data":{"text/plain":["28996"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.vocab_size"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:45:05.660489Z","iopub.status.busy":"2024-06-14T04:45:05.660010Z","iopub.status.idle":"2024-06-14T04:45:05.668458Z","shell.execute_reply":"2024-06-14T04:45:05.667412Z","shell.execute_reply.started":"2024-06-14T04:45:05.660455Z"},"trusted":true},"outputs":[],"source":["class BiDAF(nn.Module):\n","\tdef __init__(self, vocab_size, embed_size, hidden_size, c_len, BERT=False):\n","\t\tsuper(BiDAF, self).__init__()\n","\t\tself.w2v = Word2Vec(vocab_size, embed_size, BERT) # vocab_size, embed_size\n","\t\tself.qcontext = ContextualEmbedding(embed_size, hidden_size) # embed_size, hidden_size\n","\t\tself.ccontext = ContextualEmbedding(embed_size, hidden_size)\n","\t\tself.e2e = E2E(hidden_size, c_len) # hidden_size, c_len\n","\t\n","\tdef forward(self, q, c):\n","\t\tq = self.w2v(q)\n","\t\tc = self.w2v(c)\n","\t\tq = self.qcontext(q)\n","\t\tc = self.ccontext(c)\n","\t\treturn self.e2e(q, c)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:45:05.963931Z","iopub.status.busy":"2024-06-14T04:45:05.963192Z","iopub.status.idle":"2024-06-14T04:45:05.989437Z","shell.execute_reply":"2024-06-14T04:45:05.988228Z","shell.execute_reply.started":"2024-06-14T04:45:05.963897Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:45:06.338361Z","iopub.status.busy":"2024-06-14T04:45:06.337241Z","iopub.status.idle":"2024-06-14T04:45:06.343013Z","shell.execute_reply":"2024-06-14T04:45:06.342050Z","shell.execute_reply.started":"2024-06-14T04:45:06.338318Z"},"trusted":true},"outputs":[],"source":["BERT = True "]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:45:41.627183Z","iopub.status.busy":"2024-06-14T04:45:41.626816Z","iopub.status.idle":"2024-06-14T04:45:41.846498Z","shell.execute_reply":"2024-06-14T04:45:41.845653Z","shell.execute_reply.started":"2024-06-14T04:45:41.627157Z"},"trusted":true},"outputs":[],"source":["if BERT:\n","\tmodel = BiDAF(vocab_size=tokenizer.vocab_size, embed_size=100, hidden_size=100, c_len=64, BERT=True)\n","\t# model.to(device)\n","# \tmodel = torch.compile(model)\n","else:\n","\tmodel = BiDAF(vocab_size=tokenizer.vocab_size, embed_size=128, hidden_size=256, c_len=128)\n","\tmodel.to(device)\n","# \tmodel = torch.compile(model)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of trainable parameters: 7124503\n"]}],"source":["total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'Total number of trainable parameters: {total_trainable_params}')"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:45:41.969551Z","iopub.status.busy":"2024-06-14T04:45:41.968746Z","iopub.status.idle":"2024-06-14T04:45:41.974481Z","shell.execute_reply":"2024-06-14T04:45:41.973364Z","shell.execute_reply.started":"2024-06-14T04:45:41.969519Z"},"trusted":true},"outputs":[],"source":["optimizer = torch.optim.Adadelta(model.parameters(), lr = 0.5, weight_decay=0.0001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, steps_per_epoch=3274, epochs=2, anneal_strategy='cos')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3274, eta_min=1e-6)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:45:42.741464Z","iopub.status.busy":"2024-06-14T04:45:42.741080Z","iopub.status.idle":"2024-06-14T04:45:42.746194Z","shell.execute_reply":"2024-06-14T04:45:42.745280Z","shell.execute_reply.started":"2024-06-14T04:45:42.741434Z"},"trusted":true},"outputs":[],"source":["critereon = nn.CrossEntropyLoss().to('cuda')"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/plain":["2"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["a = torch.tensor([1, 0, 1])\n","b = torch.tensor([1, 11, 1])\n","(a==b).sum().item()"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:45:47.387039Z","iopub.status.busy":"2024-06-14T04:45:47.386648Z","iopub.status.idle":"2024-06-14T04:45:47.391467Z","shell.execute_reply":"2024-06-14T04:45:47.390490Z","shell.execute_reply.started":"2024-06-14T04:45:47.387008Z"},"trusted":true},"outputs":[],"source":["import time"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:45:47.687676Z","iopub.status.busy":"2024-06-14T04:45:47.687318Z","iopub.status.idle":"2024-06-14T04:45:47.696108Z","shell.execute_reply":"2024-06-14T04:45:47.695027Z","shell.execute_reply.started":"2024-06-14T04:45:47.687647Z"},"trusted":true},"outputs":[],"source":["def train(model, train_data, optimizer, critereon, scheduler, epochs=1):\n","\tstart = time.time()\n","\tfor epoch in range(epochs):\n","\t\tmodel.train()\n","\t\trunning_loss = 0.0\n","\t\tfor questions, contexts, labels in train_data:\n","\t\t\toptimizer.zero_grad(set_to_none= True)\n","\t\t\tquestions = questions.to(device)            \n","\t\t\tcontexts = contexts.to(device)\n","\t\t\tlabels = labels.long().to(device)\n","\t\t\t# with torch.autocast(device_type=device, dtype=torch.bfloat16):\n","\t\t\toutput = model(questions, contexts)\n","\t\t\tloss = critereon(output, labels)\n","\t\t\tloss.backward()\n","\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\t\t\toptimizer.step()\n","\t\t\tscheduler.step()\n","\t\t\trunning_loss += loss.item()\n","\t\tprint(f\"Epoch: {epoch}, Loss: {running_loss/len(train_data)}\")\n","\tend = time.time()\n","\tprint(f\"Training time: {end-start}\")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:45:50.247623Z","iopub.status.busy":"2024-06-14T04:45:50.246716Z","iopub.status.idle":"2024-06-14T05:19:00.844342Z","shell.execute_reply":"2024-06-14T05:19:00.843303Z","shell.execute_reply.started":"2024-06-14T04:45:50.247585Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss: 0.6946736006691052\n","Epoch: 1, Loss: 0.6942744904432815\n","Epoch: 2, Loss: 0.6941428245446933\n","Epoch: 3, Loss: 0.6936850903894954\n","Epoch: 4, Loss: 0.6936270804070055\n","Epoch: 5, Loss: 0.6939113915157014\n","Epoch: 6, Loss: 0.6936165619962893\n","Epoch: 7, Loss: 0.6929599953154786\n","Epoch: 8, Loss: 0.684400350902789\n","Epoch: 9, Loss: 0.6681230144378857\n","Epoch: 10, Loss: 0.6299331245330957\n","Epoch: 11, Loss: 0.5830079354702855\n","Epoch: 12, Loss: 0.4847485973193242\n","Epoch: 13, Loss: 0.40153783971604445\n","Epoch: 14, Loss: 0.3130649346858263\n","Epoch: 15, Loss: 0.2463168220478482\n","Epoch: 16, Loss: 0.2161488656621105\n","Epoch: 17, Loss: 0.2234264256435628\n","Epoch: 18, Loss: 0.2960725605678254\n","Epoch: 19, Loss: 0.35526410814005727\n","Epoch: 20, Loss: 0.3124506064041997\n","Epoch: 21, Loss: 0.2103419107163307\n","Epoch: 22, Loss: 0.19358109257901057\n","Epoch: 23, Loss: 0.13845096823176184\n","Epoch: 24, Loss: 0.15889592708115235\n","Epoch: 25, Loss: 0.1566587250832968\n","Epoch: 26, Loss: 0.10692773797190763\n","Epoch: 27, Loss: 0.10748318614610754\n","Epoch: 28, Loss: 0.09015156829673142\n","Epoch: 29, Loss: 0.15029281660480745\n","Epoch: 30, Loss: 0.08943705704749767\n","Epoch: 31, Loss: 0.23788471085658391\n","Training time: 1990.5919511318207\n"]}],"source":["train(model, train_data, optimizer, critereon, epochs=32)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T05:19:56.408688Z","iopub.status.busy":"2024-06-14T05:19:56.408304Z","iopub.status.idle":"2024-06-14T05:19:56.540992Z","shell.execute_reply":"2024-06-14T05:19:56.540173Z","shell.execute_reply.started":"2024-06-14T05:19:56.408659Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), \"/kaggle/working/bidaf32.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T05:21:03.400470Z","iopub.status.busy":"2024-06-14T05:21:03.399544Z","iopub.status.idle":"2024-06-14T05:21:03.411957Z","shell.execute_reply":"2024-06-14T05:21:03.410926Z","shell.execute_reply.started":"2024-06-14T05:21:03.400429Z"},"trusted":true},"outputs":[],"source":["def evaluation(model, val_data, critereon):\n","\tmodel.eval()\n","\trunning_loss = 0.0\n","\ttotal = 0\n","\tcorrect = 0\n","\t# with torch.no_grad():\n","\tfor questions, contexts, labels in val_data:\n","\t\t\tquestions = questions.to(device)\n","\t\t\tcontexts = contexts.to(device)\n","\t\t\tlabels = labels.long().to(device)\n","\t\t\twith torch.no_grad():\n","\t\t\t\t# with torch.autocast(device_type=device, dtype=torch.float16):\n","\t\t\t\toutput = model(questions, contexts)\n","\t\t\t\tloss = critereon(output.view(-1, 2), labels.view(-1))\n","\t\t\t\trunning_loss += loss.item()\n","\t\t\t\t_, predicted = torch.max(output, 1)\n","\t\t\t\ttotal += labels.size(0)\n","\t\t\t\tcorrect += (predicted == labels).sum().item()\n","\tprint(f\"Validation Loss: {running_loss/len(val_data)}\")\n","\tprint(f\"Accuracy: {100*correct/total}\")"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T05:21:05.147317Z","iopub.status.busy":"2024-06-14T05:21:05.146974Z","iopub.status.idle":"2024-06-14T05:21:07.882707Z","shell.execute_reply":"2024-06-14T05:21:07.881755Z","shell.execute_reply.started":"2024-06-14T05:21:05.147292Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Loss: 2.807593956589699\n","Accuracy: 50.5\n"]}],"source":["evaluation(model, validation_data, critereon)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T05:21:23.763321Z","iopub.status.busy":"2024-06-14T05:21:23.762462Z","iopub.status.idle":"2024-06-14T05:21:50.457289Z","shell.execute_reply":"2024-06-14T05:21:50.456321Z","shell.execute_reply.started":"2024-06-14T05:21:23.763287Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Loss: 1.6514337478163161\n","Accuracy: 66.71\n"]}],"source":["evaluation(model, train_data, critereon)"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T05:25:41.820873Z","iopub.status.busy":"2024-06-14T05:25:41.820257Z","iopub.status.idle":"2024-06-14T05:25:41.824979Z","shell.execute_reply":"2024-06-14T05:25:41.824036Z","shell.execute_reply.started":"2024-06-14T05:25:41.820833Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T05:25:22.714635Z","iopub.status.busy":"2024-06-14T05:25:22.714283Z","iopub.status.idle":"2024-06-14T05:25:22.919446Z","shell.execute_reply":"2024-06-14T05:25:22.918237Z","shell.execute_reply.started":"2024-06-14T05:25:22.714607Z"},"trusted":true},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 510.12 MiB is free. Process 2115 has 15.39 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritereon\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[30], line 13\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(model, val_data, critereon)\u001b[0m\n\u001b[1;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     12\u001b[0m \t\u001b[38;5;66;03m# with torch.autocast(device_type=device, dtype=torch.float16):\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \toutput \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \tloss \u001b[38;5;241m=\u001b[39m critereon(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     15\u001b[0m \trunning_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mBiDAF.forward\u001b[0;34m(self, q, c)\u001b[0m\n\u001b[1;32m     11\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(q)\n\u001b[1;32m     12\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(c)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 40\u001b[0m, in \u001b[0;36mE2E.forward\u001b[0;34m(self, c, q)\u001b[0m\n\u001b[1;32m     38\u001b[0m _c \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m _q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, c_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m cq \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m input_s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([_c,_q,cq], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [bs, clen, qlen, hidden_size*6]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWs(input_s)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#similarity matrix [bs, clen, qlen] \u001b[39;00m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 510.12 MiB is free. Process 2115 has 15.39 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["evaluation(model, test_data, critereon)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T05:24:41.456558Z","iopub.status.busy":"2024-06-14T05:24:41.455900Z","iopub.status.idle":"2024-06-14T05:24:41.463066Z","shell.execute_reply":"2024-06-14T05:24:41.462047Z","shell.execute_reply.started":"2024-06-14T05:24:41.456508Z"},"trusted":true},"outputs":[],"source":["def accuracy(model, val_data):\n","\tmodel.eval()\n","\tcorrect = 0\n","\ttotal = 0\n","\t# with torch.no_grad():\n","\tfor questions, contexts, labels in val_data:\n","\t\t\tquestions = questions.to(device)\n","\t\t\tcontexts = contexts.to(device)\n","\t\t\tlabels = labels.long().to(device)\n","\t\t\twith torch.no_grad():\n","\t\t\t\t# with torch.autocast(device_type=device, dtype=torch.bfloat16):\n","\t\t\t\toutput = model(questions, contexts)\n","\t\t\t\t_, predicted = torch.max(output, 1)\n","\t\t\t\ttotal += labels.size(0)\n","\t\t\t\tcorrect += (predicted == labels).sum().item()\n","\tprint(f\"Accuracy: {100*correct/total}\")"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:01:21.734611Z","iopub.status.busy":"2024-06-14T04:01:21.734306Z","iopub.status.idle":"2024-06-14T04:01:47.357526Z","shell.execute_reply":"2024-06-14T04:01:47.356604Z","shell.execute_reply.started":"2024-06-14T04:01:21.734587Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 51.65\n"]}],"source":["accuracy(model, train_data)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:01:47.359437Z","iopub.status.busy":"2024-06-14T04:01:47.359138Z","iopub.status.idle":"2024-06-14T04:01:50.073526Z","shell.execute_reply":"2024-06-14T04:01:50.072502Z","shell.execute_reply.started":"2024-06-14T04:01:47.359411Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 47.9\n"]}],"source":["accuracy(model, validation_data)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T05:25:58.126290Z","iopub.status.busy":"2024-06-14T05:25:58.125677Z","iopub.status.idle":"2024-06-14T05:25:58.541664Z","shell.execute_reply":"2024-06-14T05:25:58.540309Z","shell.execute_reply.started":"2024-06-14T05:25:58.126260Z"},"trusted":true},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 15.89 GiB of which 510.12 MiB is free. Process 2115 has 15.39 GiB memory in use. Of the allocated memory 14.55 GiB is allocated by PyTorch, and 540.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[34], line 12\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(model, val_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m \t\u001b[38;5;66;03m# with torch.autocast(device_type=device, dtype=torch.bfloat16):\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \toutput \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \t_, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \ttotal \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mBiDAF.forward\u001b[0;34m(self, q, c)\u001b[0m\n\u001b[1;32m     11\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(q)\n\u001b[1;32m     12\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(c)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mE2E.forward\u001b[0;34m(self, c, q)\u001b[0m\n\u001b[1;32m     39\u001b[0m _q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, c_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m cq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(_c,_q)\n\u001b[0;32m---> 41\u001b[0m input_s \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [bs, clen, qlen, hidden_size*6]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWs(input_s)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#similarity matrix [bs, clen, qlen] \u001b[39;00m\n\u001b[1;32m     45\u001b[0m s1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(s, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 15.89 GiB of which 510.12 MiB is free. Process 2115 has 15.39 GiB memory in use. Of the allocated memory 14.55 GiB is allocated by PyTorch, and 540.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["accuracy(model, test_data)"]},{"cell_type":"code","execution_count":33,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-14T04:09:33.573188Z","iopub.status.busy":"2024-06-14T04:09:33.572582Z","iopub.status.idle":"2024-06-14T04:09:33.633901Z","shell.execute_reply":"2024-06-14T04:09:33.633049Z","shell.execute_reply.started":"2024-06-14T04:09:33.573158Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[  101,  5979,  2360,  3648,  1174,  1113,   170,  1686,  1683,  1104,\n","          2454,   112,   188,  1109,  3237, 14303,  3414,  1212,   136,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101, 23963,  1233,   179, 11470, 23085,  1320,  1400,  2017,  1107,\n","          1402,  1390,  1170,  4510,  2133,  1390,   136,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327, 10209,  1521,  1106,  1103,  9711,  1104, 10579,   112,\n","           188, 14781,   136,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  5979, 10939,  1107,  1103,  1646,  1105,  2855,  1253, 12912,\n","          1103,  8832,  1947,  1111,  2029, 15417,   136,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  2627,  1108,  2784,  1111,  8315,  1158,   170,  1207,  1440,\n","          1111,  1155,  7302,  2982,   136,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327,  1108,  1103,  1314,  1835,  2208,  4498,  3567,   136,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  7187,  1103,  2384,  1104,  2270,  1329,  1103,  1418,  8843,\n","          1858,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327,  2076,  1104,  1390,  1202,   143,  5658, 23928,  1105,\n","          7206, 21245,  3870,   136,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  3743,  1104,  1515,  1126, 21585, 12661,   117,  1184,  1674,\n","         25994,  2059,   136,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1731,  1242,  1703,  1638,  1674,  1296,  1526,  1107,  1103,\n","          3863,  1453,  1505,  1679,  1265,   136,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327,  7181,  1225,  1103, 13286,  7731, 13639,   136,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  2627,  1108,  3406,   112,   188,  1534,   136,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327,  1110,  1103,  1271,  1549,  1106,  1103,  5083,  3488,\n","         15253,  1107,  1103, 20104,  1477,  3168,  1104,  6869, 16203,   136,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  5979,  4128,  1108,  2628,  1114,  1103,  5945,  1836,   136,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1130,  1184,  1432,  1225,  1103,  8328,  1947,  1148, 27760,\n","           136,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327,  1202, 24757,  2227,  7004, 24819,  1942,  4651,   136,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  4823,  2942,  1110,  1510,  1270,  1184,  1165,  7455,  1106,\n","          6363,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,   155,  2285, 24405,  4491,  1111,   170,  2808,  1359,  1113,\n","          1184,   117,  1897,  1190,  5228,   136,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327,  5376,  1112,   170,  2952, 20382,  1106,  2848,   118,\n","          2163,  6465,   136,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327, 20343,  1108,  9142,  9308,  1111,  1535,   136,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  2102,  1741,  5027, 18025,  1107,  1184,  5027,   136,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327,  1108,  1103, 11242,  1104, 25702, 14410, 24648,  6763,\n","           136,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1731,  1263,  1108,  1103, 27153,  1104, 23295,  1107,  1329,\n","          1111,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327,  1168,  2505,  2523,  1225,   140, 17312,  6582,  7221,\n","          2904,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  9854, 20164, 15114,   112,   188,  6004,  1164, 26341,  2148,\n","          1111,  1134,  2749,  1118, 10792,  3180,  1918,  2875,  1874,   136,\n","           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327,  1110,  1103,  1248,  1211,  1887,  7309,  1372,  1107,\n","         23761,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327,  1250,  1225,  1103,  3167,  5996,  1111,  1335, 17013,\n","         20478,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  3982,  1103,  1503,  3884,  1104,  1103,  1148,  2596,  8012,\n","          6664,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1130, 12681,  1150,  1225,  3414, 23870,  1116,  3326,  1114,\n","          1103,  1494,  1104,  5586, 16588,   136,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  1327, 10548,  1108,  4379,  1112,  1196,  5250,  6610,  7490,\n","          2941,   172, 25669,  2155, 13622, 22273,  1108,  2751,   136,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0],\n","        [  101,  2777,  1110,  1103,  2683,  2859,  2334,  3782,   136,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(dataset['train'][2269:2300]['question'], max_length =32, padding='max_length', truncation=True, return_tensors='pt').to('cuda')"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:11:56.836949Z","iopub.status.busy":"2024-06-14T04:11:56.836560Z","iopub.status.idle":"2024-06-14T04:11:56.843479Z","shell.execute_reply":"2024-06-14T04:11:56.842497Z","shell.execute_reply.started":"2024-06-14T04:11:56.836917Z"},"trusted":true},"outputs":[],"source":["def predict(model, questions, contexts,labels, tokenizer, max_len):\n","    questions = tokenizer(questions, max_length = max_len, padding='max_length', truncation=True, return_tensors='pt')['input_ids'].to('cuda')\n","    contexts = tokenizer(contexts, max_length = max_len, padding='max_length', truncation=True, return_tensors='pt')['input_ids'].to('cuda')\n","    with torch.no_grad():\n","        output = model(questions, contexts)\n","        _, predicted = torch.max(output, 1)\n","    print(\"predicted label :\", predicted)\n","    print(\"actual label :\", labels)"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:12:00.386941Z","iopub.status.busy":"2024-06-14T04:12:00.386555Z","iopub.status.idle":"2024-06-14T04:12:00.486447Z","shell.execute_reply":"2024-06-14T04:12:00.485447Z","shell.execute_reply.started":"2024-06-14T04:12:00.386912Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["predicted label : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n","        1, 1, 0, 0, 1, 1, 0], device='cuda:0')\n","actual label : [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0]\n"]}],"source":["predict(model, dataset['train'][2269:2300]['question'], dataset['train'][2269:2300]['sentence'],dataset['train'][2269:2300]['label'], tokenizer, 128)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:11:50.478843Z","iopub.status.busy":"2024-06-14T04:11:50.478044Z","iopub.status.idle":"2024-06-14T04:11:50.487607Z","shell.execute_reply":"2024-06-14T04:11:50.486447Z","shell.execute_reply.started":"2024-06-14T04:11:50.478803Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["dataset['train'][2269:2300]['label']"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:16:57.025097Z","iopub.status.busy":"2024-06-14T04:16:57.024201Z","iopub.status.idle":"2024-06-14T04:16:57.030036Z","shell.execute_reply":"2024-06-14T04:16:57.028843Z","shell.execute_reply.started":"2024-06-14T04:16:57.025058Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T04:17:20.522956Z","iopub.status.busy":"2024-06-14T04:17:20.522353Z","iopub.status.idle":"2024-06-14T04:17:20.744202Z","shell.execute_reply":"2024-06-14T04:17:20.743010Z","shell.execute_reply.started":"2024-06-14T04:17:20.522922Z"},"trusted":true},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 4.69 GiB. GPU 0 has a total capacty of 15.89 GiB of which 598.12 MiB is free. Process 2110 has 15.31 GiB memory in use. Of the allocated memory 14.72 GiB is allocated by PyTorch, and 283.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2600\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2600\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2600\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2650\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2600\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2650\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[45], line 5\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, questions, contexts, labels, tokenizer, max_len)\u001b[0m\n\u001b[1;32m      3\u001b[0m contexts \u001b[38;5;241m=\u001b[39m tokenizer(contexts, max_length \u001b[38;5;241m=\u001b[39m max_len, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted label :\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mBiDAF.forward\u001b[0;34m(self, q, c)\u001b[0m\n\u001b[1;32m     11\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(q)\n\u001b[1;32m     12\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(c)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mE2E.forward\u001b[0;34m(self, c, q)\u001b[0m\n\u001b[1;32m     39\u001b[0m _q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, c_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m cq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(_c,_q)\n\u001b[0;32m---> 41\u001b[0m input_s \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [bs, clen, qlen, hidden_size*6]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWs(input_s)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#similarity matrix [bs, clen, qlen] \u001b[39;00m\n\u001b[1;32m     45\u001b[0m s1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(s, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.69 GiB. GPU 0 has a total capacty of 15.89 GiB of which 598.12 MiB is free. Process 2110 has 15.31 GiB memory in use. Of the allocated memory 14.72 GiB is allocated by PyTorch, and 283.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["predict(model, dataset['train'][2600:(2600+50)]['question'], dataset['train'][2600:2650]['sentence'],dataset['train'][2600:2650]['label'], tokenizer, 128)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
