{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import BertTokenizerFast, BertModel\n# from embed_layer import Word2Vec, ContextualEmbedding\n# from e2e import E2E \n# from data import SquadDataset\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:44:18.025815Z","iopub.execute_input":"2024-06-14T04:44:18.026446Z","iopub.status.idle":"2024-06-14T04:44:24.362892Z","shell.execute_reply.started":"2024-06-14T04:44:18.026415Z","shell.execute_reply":"2024-06-14T04:44:24.362139Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer\n\nclass SquadDataset(torch.utils.data.Dataset):\n\t'''\n\t- Creates batches dynamically by padding to the length of largest example\n\t  in a given batch.\n\t- Calulates character vectors for contexts and question.\n\t- Returns tensors for training.\n\t'''\n\t\n\tdef __init__(self, data, batch_size, tokenizer, max_len):\n\t\t\n\t\tself.batch_size = batch_size\n\t\tdata = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n\t\tself.data = data\n\t\t# self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\t\tself.tokenizer = tokenizer\n\t\tself.max_len = max_len\n\t\t\n\tdef __len__(self):\n\t\treturn len(self.data)\n\t\n\tdef __iter__(self):\n\t\t'''\n\t\tCreates batches of data and yields them.\n\t\t\n\t\tEach yield comprises of:\n\t\t:padded_context: padded tensor of contexts for each batch \n\t\t:padded_question: padded tensor of questions for each batch \n\t\t:label: \n\t\t\n\t\t'''\n\t\t\n\t\tfor batch in self.data:\n\t\t\tquestions = self.tokenizer(batch['question'], max_length = self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n\t\t\tcontexts = self.tokenizer(batch['sentence'], max_length = self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n\t\t\tlabels = torch.IntTensor(batch['label']).to(torch.int8)\n\t\t\t# question, context include input_ids, attention_mask, token_type_ids\n\t\t\tyield questions['input_ids'], contexts['input_ids'], labels","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:44:24.364806Z","iopub.execute_input":"2024-06-14T04:44:24.365385Z","iopub.status.idle":"2024-06-14T04:44:24.375949Z","shell.execute_reply.started":"2024-06-14T04:44:24.365357Z","shell.execute_reply":"2024-06-14T04:44:24.374917Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import BertModel\n\nclass Word2Vec(nn.Module):\n\tdef __init__(self, vocab_size, embed_size, BERT = False): \n\t\tsuper(Word2Vec, self).__init__()\n\t\tif BERT:\n\t\t\tmodel = BertModel.from_pretrained('bert-base-uncased')\n\t\t\tself.embeddings = model.embeddings.word_embeddings\n\t\t\tself.embeddings.requires_grad_(False)\n\t\telse:\t\n\t\t\tself.embeddings = nn.Embedding(vocab_size, embed_size)\n\t\t\ttorch.nn.init.normal_(self.embeddings.weight, mean=0, std=0.02)\n\tdef forward(self, x):\n\t\tx = self.embeddings(x)\n\t\treturn x\n\n# class Highway(nn.Module):\n# \tdef __init__(self, args):\n# \t\tsuper(Highway, self).__init__()\n# \t\tself.W_proj = nn.Linear(args.hidden_size*2, args.hidden_size*2)\n# \t\tself.W_gate = nn.Linear(args.hidden_size, args.hidden_size)\n\t\n# \tdef forward(self, x):\n# \t\tx_proj = F.relu(self.W_proj(x))\n# \t\tx_gate = F.sigmoid(self.W_gate(x))\n# \t\tx_highway = x_gate * x_proj + (1 - x_gate) * x\n# \t\treturn x_highway\n\nclass ContextualEmbedding(nn.Module):\n\tdef __init__(self, embed_size, hidden_size):\n\t\tsuper(ContextualEmbedding, self).__init__()\n\t\tself.RNN= nn.RNN(input_size=embed_size,\n\t\t\t\t\t\t\thidden_size=hidden_size,\n\t\t\t\t\t\t\tnum_layers=2,\n\t\t\t\t\t\t\tbidirectional=True,\n\t\t\t\t\t\t\tbatch_first=True,\n\t\t\t\t\t\t\tdropout=0.2)\n\tdef forward(self, x):\n\t\toutput, _ = self.RNN(x)\n\t\treturn output\n\t\t","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:44:24.377175Z","iopub.execute_input":"2024-06-14T04:44:24.377453Z","iopub.status.idle":"2024-06-14T04:44:24.389257Z","shell.execute_reply.started":"2024-06-14T04:44:24.377428Z","shell.execute_reply":"2024-06-14T04:44:24.388245Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass E2E(nn.Module):\n    def __init__(self, hidden_size, c_len):\n        super(E2E, self).__init__()\n        '''\n        input: [bs, qlen, hidden_size*2], [bs, clen, hidden_size*2]\n        1. We need to calculate the similarity matrix between the context and the query \n        2. We need to calculate the attention weights for the context and the query [bs, clen, qlen], and then caculate c2q by multiply [bs, clen, qlen] with [bs, qlen, hidden_size*2] is called c2q\n        3. We need to calculate the attention weights for the query and the context [bs, qlen, clen], and then caculate q2c by multiply [bs, qlen, clen] with [bs, clen, hidden_size*2] is called q2c\n        4. Then we concat [context, q2c, context*q2c, ] \n\n        '''\n        self.Ws = nn.Linear(hidden_size*6, 1, bias=False)\n        torch.nn.init.normal_(self.Ws.weight, mean=0, std=0.02)\n\n        self.rnn = nn.RNN(input_size=hidden_size*8, hidden_size=hidden_size*4, num_layers=2, bidirectional=True, batch_first=True, dropout=0.2)\n        self.dropout = nn.Dropout(0.1)\n\n        self.last1 = nn.Linear(hidden_size*8, 1, bias=False)\n        self.last2 = nn.Linear(c_len, 2, bias=False)\n        \n        torch.nn.init.normal_(self.last1.weight, mean=0, std=0.02)\n        torch.nn.init.normal_(self.last2.weight, mean=0, std=0.02)\n    def forward(self, c, q):\n        '''\n        c: [bs, clen, hidden_size*2]\n        q: [bs, qlen, hidden_size*2]\n\n        '''\n        bs = c.size(0)\n        c_len = c.size(1)\n        q_len = q.size(1)\n        hidden_size = c.size(2)\n\n        _c = c.unsqueeze(2).expand(-1, -1, q_len, -1)\n        _q = q.unsqueeze(1).expand(-1, c_len, -1, -1)\n        cq = torch.mul(_c,_q)\n        input_s = torch.cat([_c,_q,cq], dim=-1) # [bs, clen, qlen, hidden_size*6]\n\n        s = self.Ws(input_s).squeeze(-1) #similarity matrix [bs, clen, qlen] \n\n        s1 = F.softmax(s, dim=-1)\n        c2q = torch.bmm(s1, q) # [bs, clen, hidden_size*2], cco the hieu la ta bieu dien cac word trong context bang to hop attention_score*query\n        c2q = self.dropout(c2q) \n        \n        #q2c\n        s2 = F.softmax(torch.max(s, dim=-1)[0], dim=-1) # [bs, clen]\n        s2 = s2.unsqueeze(1).expand(-1, q_len, -1) # [bs, qlen, clen]\n        q2c = torch.bmm(s2, c) # [bs, qlen, hidden_size*2]\n        q2c = self.dropout(q2c) \n\n        #querry-aware representation \n        G = torch.cat([c, c2q, torch.mul(c, c2q), torch.mul(c, q2c)], dim=-1)    # [bs, clen, hidden_size*8]\n        M, _ = self.rnn(G) # [bs, clen, hidden_size*8]\n        M = self.dropout(M) \n        M = M + G # residual connection\n        out1 = self.last1(M) # [bs, clen, 1]\n        out1 = out1.squeeze(-1) # [bs, clen]\n        out1 = self.dropout(out1)\n        out2 = self.last2(out1)\n        return out2\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:44:24.391113Z","iopub.execute_input":"2024-06-14T04:44:24.391965Z","iopub.status.idle":"2024-06-14T04:44:24.407393Z","shell.execute_reply.started":"2024-06-14T04:44:24.391939Z","shell.execute_reply":"2024-06-14T04:44:24.406529Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndataset = load_dataset(\"nyu-mll/glue\", \"qnli\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:44:24.408355Z","iopub.execute_input":"2024-06-14T04:44:24.408605Z","iopub.status.idle":"2024-06-14T04:44:37.402056Z","shell.execute_reply.started":"2024-06-14T04:44:24.408583Z","shell.execute_reply":"2024-06-14T04:44:37.401072Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"699b7b4c78ab4509b451b2624024e03c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"461406ac95934fc29180d41a71c3c545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb97c83a65b44c218f8b70e7617e1d56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/877k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c884b41dba54d18a9151c737cddd086"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/104743 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90126247e3074c6cb62b469f312c1a46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/5463 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aecbd54e88024cb2b9227ae0a61dc9b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5463 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c566667df6946a886c6bdf24f2c86ad"}},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:44:37.403410Z","iopub.execute_input":"2024-06-14T04:44:37.404288Z","iopub.status.idle":"2024-06-14T04:44:37.412742Z","shell.execute_reply.started":"2024-06-14T04:44:37.404247Z","shell.execute_reply":"2024-06-14T04:44:37.411880Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'question': 'When did the third Digimon series begin?',\n 'sentence': 'Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese.',\n 'label': 1,\n 'idx': 0}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:44:37.413958Z","iopub.execute_input":"2024-06-14T04:44:37.414299Z","iopub.status.idle":"2024-06-14T04:44:40.307315Z","shell.execute_reply.started":"2024-06-14T04:44:37.414269Z","shell.execute_reply":"2024-06-14T04:44:40.306331Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c217c44c128e4cfcbc152a2f79e0f131"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c50797bbd8d14cc795b58c995104ed41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab5321f796394f7e98c618ff19f527a6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9621d0812d741d29c4953bdbb740e39"}},"metadata":{}}]},{"cell_type":"code","source":"random_train = dataset['train'].select(range(2269,12269))\nrandom_val = dataset['validation'].select(range(2269,3269))\nrandom_test = dataset['validation'].select(range(3269,4269))","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:44:40.309547Z","iopub.execute_input":"2024-06-14T04:44:40.309921Z","iopub.status.idle":"2024-06-14T04:44:40.325485Z","shell.execute_reply.started":"2024-06-14T04:44:40.309889Z","shell.execute_reply":"2024-06-14T04:44:40.324553Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_data = SquadDataset(random_train, 32, tokenizer, 128)\nvalidation_data = SquadDataset(random_val, 32, tokenizer, 128)\ntest_data = SquadDataset(random_test, 16, tokenizer, 128)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:25:49.570122Z","iopub.execute_input":"2024-06-14T05:25:49.570742Z","iopub.status.idle":"2024-06-14T05:25:49.667231Z","shell.execute_reply.started":"2024-06-14T05:25:49.570705Z","shell.execute_reply":"2024-06-14T05:25:49.666255Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"tokenizer.vocab_size","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:45:05.188646Z","iopub.execute_input":"2024-06-14T04:45:05.189314Z","iopub.status.idle":"2024-06-14T04:45:05.195704Z","shell.execute_reply.started":"2024-06-14T04:45:05.189282Z","shell.execute_reply":"2024-06-14T04:45:05.194681Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"28996"},"metadata":{}}]},{"cell_type":"code","source":"class BiDAF(nn.Module):\n\tdef __init__(self, vocab_size, embed_size, hidden_size, c_len, BERT=False):\n\t\tsuper(BiDAF, self).__init__()\n\t\tself.w2v = Word2Vec(vocab_size, embed_size, BERT) # vocab_size, embed_size\n\t\tself.acontext = ContextualEmbedding(embed_size, hidden_size) # embed_size, hidden_size\n\t\tself.e2e = E2E(hidden_size, c_len) # hidden_size, c_len\n\t\n\tdef forward(self, q, c):\n\t\tq = self.w2v(q)\n\t\tc = self.w2v(c)\n\t\tq = self.acontext(q)\n\t\tc = self.acontext(c)\n\t\treturn self.e2e(q, c)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:45:05.660010Z","iopub.execute_input":"2024-06-14T04:45:05.660489Z","iopub.status.idle":"2024-06-14T04:45:05.668458Z","shell.execute_reply.started":"2024-06-14T04:45:05.660455Z","shell.execute_reply":"2024-06-14T04:45:05.667412Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:45:05.963192Z","iopub.execute_input":"2024-06-14T04:45:05.963931Z","iopub.status.idle":"2024-06-14T04:45:05.989437Z","shell.execute_reply.started":"2024-06-14T04:45:05.963897Z","shell.execute_reply":"2024-06-14T04:45:05.988228Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"BERT = False","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:45:06.337241Z","iopub.execute_input":"2024-06-14T04:45:06.338361Z","iopub.status.idle":"2024-06-14T04:45:06.343013Z","shell.execute_reply.started":"2024-06-14T04:45:06.338318Z","shell.execute_reply":"2024-06-14T04:45:06.342050Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"if BERT:\n\tmodel = BiDAF(vocab_size=tokenizer.vocab_size, embed_size=768, hidden_size=256, c_len=128, BERT=True)\n\tmodel.to(device)\n# \tmodel = torch.compile(model)\nelse:\n\tmodel = BiDAF(vocab_size=tokenizer.vocab_size, embed_size=128, hidden_size=256, c_len=128)\n\tmodel.to(device)\n# \tmodel = torch.compile(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:45:41.626816Z","iopub.execute_input":"2024-06-14T04:45:41.627183Z","iopub.status.idle":"2024-06-14T04:45:41.846498Z","shell.execute_reply.started":"2024-06-14T04:45:41.627157Z","shell.execute_reply":"2024-06-14T04:45:41.845653Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adadelta(model.parameters(), lr = 0.5, weight_decay=0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:45:41.968746Z","iopub.execute_input":"2024-06-14T04:45:41.969551Z","iopub.status.idle":"2024-06-14T04:45:41.974481Z","shell.execute_reply.started":"2024-06-14T04:45:41.969519Z","shell.execute_reply":"2024-06-14T04:45:41.973364Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"critereon = nn.CrossEntropyLoss().to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:45:42.741080Z","iopub.execute_input":"2024-06-14T04:45:42.741464Z","iopub.status.idle":"2024-06-14T04:45:42.746194Z","shell.execute_reply.started":"2024-06-14T04:45:42.741434Z","shell.execute_reply":"2024-06-14T04:45:42.745280Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:45:47.386648Z","iopub.execute_input":"2024-06-14T04:45:47.387039Z","iopub.status.idle":"2024-06-14T04:45:47.391467Z","shell.execute_reply.started":"2024-06-14T04:45:47.387008Z","shell.execute_reply":"2024-06-14T04:45:47.390490Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def train(model, train_data, optimizer, critereon, epochs=1):\n\tstart = time.time()\n\tfor epoch in range(epochs):\n\t\tmodel.train()\n\t\trunning_loss = 0.0\n\t\tfor questions, contexts, labels in train_data:\n\t\t\toptimizer.zero_grad()\n\t\t\tquestions = questions.to(device)            \n\t\t\tcontexts = contexts.to(device)\n\t\t\tlabels = labels.long().to(device)\n\t\t\t# with torch.autocast(device_type=device, dtype=torch.bfloat16):\n\t\t\toutput = model(questions, contexts)\n\t\t\tloss = critereon(output.view(-1, 2), labels.view(-1))\n\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\t\t\ttorch.cuda.synchronize()\n\t\t\trunning_loss += loss.item()\n\t\tprint(f\"Epoch: {epoch}, Loss: {running_loss/len(train_data)}\")\n\tend = time.time()\n\tprint(f\"Training time: {end-start}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:45:47.687318Z","iopub.execute_input":"2024-06-14T04:45:47.687676Z","iopub.status.idle":"2024-06-14T04:45:47.696108Z","shell.execute_reply.started":"2024-06-14T04:45:47.687647Z","shell.execute_reply":"2024-06-14T04:45:47.695027Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train(model, train_data, optimizer, critereon, epochs=32)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:45:50.246716Z","iopub.execute_input":"2024-06-14T04:45:50.247623Z","iopub.status.idle":"2024-06-14T05:19:00.844342Z","shell.execute_reply.started":"2024-06-14T04:45:50.247585Z","shell.execute_reply":"2024-06-14T05:19:00.843303Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch: 0, Loss: 0.6946736006691052\nEpoch: 1, Loss: 0.6942744904432815\nEpoch: 2, Loss: 0.6941428245446933\nEpoch: 3, Loss: 0.6936850903894954\nEpoch: 4, Loss: 0.6936270804070055\nEpoch: 5, Loss: 0.6939113915157014\nEpoch: 6, Loss: 0.6936165619962893\nEpoch: 7, Loss: 0.6929599953154786\nEpoch: 8, Loss: 0.684400350902789\nEpoch: 9, Loss: 0.6681230144378857\nEpoch: 10, Loss: 0.6299331245330957\nEpoch: 11, Loss: 0.5830079354702855\nEpoch: 12, Loss: 0.4847485973193242\nEpoch: 13, Loss: 0.40153783971604445\nEpoch: 14, Loss: 0.3130649346858263\nEpoch: 15, Loss: 0.2463168220478482\nEpoch: 16, Loss: 0.2161488656621105\nEpoch: 17, Loss: 0.2234264256435628\nEpoch: 18, Loss: 0.2960725605678254\nEpoch: 19, Loss: 0.35526410814005727\nEpoch: 20, Loss: 0.3124506064041997\nEpoch: 21, Loss: 0.2103419107163307\nEpoch: 22, Loss: 0.19358109257901057\nEpoch: 23, Loss: 0.13845096823176184\nEpoch: 24, Loss: 0.15889592708115235\nEpoch: 25, Loss: 0.1566587250832968\nEpoch: 26, Loss: 0.10692773797190763\nEpoch: 27, Loss: 0.10748318614610754\nEpoch: 28, Loss: 0.09015156829673142\nEpoch: 29, Loss: 0.15029281660480745\nEpoch: 30, Loss: 0.08943705704749767\nEpoch: 31, Loss: 0.23788471085658391\nTraining time: 1990.5919511318207\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/bidaf32.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:19:56.408304Z","iopub.execute_input":"2024-06-14T05:19:56.408688Z","iopub.status.idle":"2024-06-14T05:19:56.540992Z","shell.execute_reply.started":"2024-06-14T05:19:56.408659Z","shell.execute_reply":"2024-06-14T05:19:56.540173Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation(model, val_data, critereon):\n\tmodel.eval()\n\trunning_loss = 0.0\n\ttotal = 0\n\tcorrect = 0\n\t# with torch.no_grad():\n\tfor questions, contexts, labels in val_data:\n\t\t\tquestions = questions.to(device)\n\t\t\tcontexts = contexts.to(device)\n\t\t\tlabels = labels.long().to(device)\n\t\t\twith torch.no_grad():\n\t\t\t\t# with torch.autocast(device_type=device, dtype=torch.float16):\n\t\t\t\toutput = model(questions, contexts)\n\t\t\t\tloss = critereon(output.view(-1, 2), labels.view(-1))\n\t\t\t\trunning_loss += loss.item()\n\t\t\t\t_, predicted = torch.max(output, 1)\n\t\t\t\ttotal += labels.size(0)\n\t\t\t\tcorrect += (predicted == labels).sum().item()\n\tprint(f\"Validation Loss: {running_loss/len(val_data)}\")\n\tprint(f\"Accuracy: {100*correct/total}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:21:03.399544Z","iopub.execute_input":"2024-06-14T05:21:03.400470Z","iopub.status.idle":"2024-06-14T05:21:03.411957Z","shell.execute_reply.started":"2024-06-14T05:21:03.400429Z","shell.execute_reply":"2024-06-14T05:21:03.410926Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"evaluation(model, validation_data, critereon)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:21:05.146974Z","iopub.execute_input":"2024-06-14T05:21:05.147317Z","iopub.status.idle":"2024-06-14T05:21:07.882707Z","shell.execute_reply.started":"2024-06-14T05:21:05.147292Z","shell.execute_reply":"2024-06-14T05:21:07.881755Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Validation Loss: 2.807593956589699\nAccuracy: 50.5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"evaluation(model, train_data, critereon)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:21:23.762462Z","iopub.execute_input":"2024-06-14T05:21:23.763321Z","iopub.status.idle":"2024-06-14T05:21:50.457289Z","shell.execute_reply.started":"2024-06-14T05:21:23.763287Z","shell.execute_reply":"2024-06-14T05:21:50.456321Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Validation Loss: 1.6514337478163161\nAccuracy: 66.71\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:25:41.820257Z","iopub.execute_input":"2024-06-14T05:25:41.820873Z","iopub.status.idle":"2024-06-14T05:25:41.824979Z","shell.execute_reply.started":"2024-06-14T05:25:41.820833Z","shell.execute_reply":"2024-06-14T05:25:41.824036Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"evaluation(model, test_data, critereon)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:25:22.714283Z","iopub.execute_input":"2024-06-14T05:25:22.714635Z","iopub.status.idle":"2024-06-14T05:25:22.919446Z","shell.execute_reply.started":"2024-06-14T05:25:22.714607Z","shell.execute_reply":"2024-06-14T05:25:22.918237Z"},"trusted":true},"execution_count":40,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritereon\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[30], line 13\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(model, val_data, critereon)\u001b[0m\n\u001b[1;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     12\u001b[0m \t\u001b[38;5;66;03m# with torch.autocast(device_type=device, dtype=torch.float16):\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \toutput \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \tloss \u001b[38;5;241m=\u001b[39m critereon(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     15\u001b[0m \trunning_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mBiDAF.forward\u001b[0;34m(self, q, c)\u001b[0m\n\u001b[1;32m     11\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(q)\n\u001b[1;32m     12\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(c)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 40\u001b[0m, in \u001b[0;36mE2E.forward\u001b[0;34m(self, c, q)\u001b[0m\n\u001b[1;32m     38\u001b[0m _c \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m _q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, c_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m cq \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m input_s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([_c,_q,cq], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [bs, clen, qlen, hidden_size*6]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWs(input_s)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#similarity matrix [bs, clen, qlen] \u001b[39;00m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 510.12 MiB is free. Process 2115 has 15.39 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 510.12 MiB is free. Process 2115 has 15.39 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"def accuracy(model, val_data):\n\tmodel.eval()\n\tcorrect = 0\n\ttotal = 0\n\t# with torch.no_grad():\n\tfor questions, contexts, labels in val_data:\n\t\t\tquestions = questions.to(device)\n\t\t\tcontexts = contexts.to(device)\n\t\t\tlabels = labels.long().to(device)\n\t\t\twith torch.no_grad():\n\t\t\t\t# with torch.autocast(device_type=device, dtype=torch.bfloat16):\n\t\t\t\toutput = model(questions, contexts)\n\t\t\t\t_, predicted = torch.max(output, 1)\n\t\t\t\ttotal += labels.size(0)\n\t\t\t\tcorrect += (predicted == labels).sum().item()\n\tprint(f\"Accuracy: {100*correct/total}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:24:41.455900Z","iopub.execute_input":"2024-06-14T05:24:41.456558Z","iopub.status.idle":"2024-06-14T05:24:41.463066Z","shell.execute_reply.started":"2024-06-14T05:24:41.456508Z","shell.execute_reply":"2024-06-14T05:24:41.462047Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"accuracy(model, train_data)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:01:21.734306Z","iopub.execute_input":"2024-06-14T04:01:21.734611Z","iopub.status.idle":"2024-06-14T04:01:47.357526Z","shell.execute_reply.started":"2024-06-14T04:01:21.734587Z","shell.execute_reply":"2024-06-14T04:01:47.356604Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Accuracy: 51.65\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy(model, validation_data)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:01:47.359138Z","iopub.execute_input":"2024-06-14T04:01:47.359437Z","iopub.status.idle":"2024-06-14T04:01:50.073526Z","shell.execute_reply.started":"2024-06-14T04:01:47.359411Z","shell.execute_reply":"2024-06-14T04:01:50.072502Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Accuracy: 47.9\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy(model, test_data)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:25:58.125677Z","iopub.execute_input":"2024-06-14T05:25:58.126290Z","iopub.status.idle":"2024-06-14T05:25:58.541664Z","shell.execute_reply.started":"2024-06-14T05:25:58.126260Z","shell.execute_reply":"2024-06-14T05:25:58.540309Z"},"trusted":true},"execution_count":43,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[34], line 12\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(model, val_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m \t\u001b[38;5;66;03m# with torch.autocast(device_type=device, dtype=torch.bfloat16):\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \toutput \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \t_, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \ttotal \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mBiDAF.forward\u001b[0;34m(self, q, c)\u001b[0m\n\u001b[1;32m     11\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(q)\n\u001b[1;32m     12\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(c)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mE2E.forward\u001b[0;34m(self, c, q)\u001b[0m\n\u001b[1;32m     39\u001b[0m _q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, c_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m cq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(_c,_q)\n\u001b[0;32m---> 41\u001b[0m input_s \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [bs, clen, qlen, hidden_size*6]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWs(input_s)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#similarity matrix [bs, clen, qlen] \u001b[39;00m\n\u001b[1;32m     45\u001b[0m s1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(s, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 15.89 GiB of which 510.12 MiB is free. Process 2115 has 15.39 GiB memory in use. Of the allocated memory 14.55 GiB is allocated by PyTorch, and 540.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacty of 15.89 GiB of which 510.12 MiB is free. Process 2115 has 15.39 GiB memory in use. Of the allocated memory 14.55 GiB is allocated by PyTorch, and 540.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"tokenizer(dataset['train'][2269:2300]['question'], max_length =32, padding='max_length', truncation=True, return_tensors='pt').to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:09:33.572582Z","iopub.execute_input":"2024-06-14T04:09:33.573188Z","iopub.status.idle":"2024-06-14T04:09:33.633901Z","shell.execute_reply.started":"2024-06-14T04:09:33.573158Z","shell.execute_reply":"2024-06-14T04:09:33.633049Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  5979,  2360,  3648,  1174,  1113,   170,  1686,  1683,  1104,\n          2454,   112,   188,  1109,  3237, 14303,  3414,  1212,   136,   102,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101, 23963,  1233,   179, 11470, 23085,  1320,  1400,  2017,  1107,\n          1402,  1390,  1170,  4510,  2133,  1390,   136,   102,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327, 10209,  1521,  1106,  1103,  9711,  1104, 10579,   112,\n           188, 14781,   136,   102,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  5979, 10939,  1107,  1103,  1646,  1105,  2855,  1253, 12912,\n          1103,  8832,  1947,  1111,  2029, 15417,   136,   102,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  2627,  1108,  2784,  1111,  8315,  1158,   170,  1207,  1440,\n          1111,  1155,  7302,  2982,   136,   102,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327,  1108,  1103,  1314,  1835,  2208,  4498,  3567,   136,\n           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  7187,  1103,  2384,  1104,  2270,  1329,  1103,  1418,  8843,\n          1858,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327,  2076,  1104,  1390,  1202,   143,  5658, 23928,  1105,\n          7206, 21245,  3870,   136,   102,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  3743,  1104,  1515,  1126, 21585, 12661,   117,  1184,  1674,\n         25994,  2059,   136,   102,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1731,  1242,  1703,  1638,  1674,  1296,  1526,  1107,  1103,\n          3863,  1453,  1505,  1679,  1265,   136,   102,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327,  7181,  1225,  1103, 13286,  7731, 13639,   136,   102,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  2627,  1108,  3406,   112,   188,  1534,   136,   102,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327,  1110,  1103,  1271,  1549,  1106,  1103,  5083,  3488,\n         15253,  1107,  1103, 20104,  1477,  3168,  1104,  6869, 16203,   136,\n           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  5979,  4128,  1108,  2628,  1114,  1103,  5945,  1836,   136,\n           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1130,  1184,  1432,  1225,  1103,  8328,  1947,  1148, 27760,\n           136,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327,  1202, 24757,  2227,  7004, 24819,  1942,  4651,   136,\n           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  4823,  2942,  1110,  1510,  1270,  1184,  1165,  7455,  1106,\n          6363,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,   155,  2285, 24405,  4491,  1111,   170,  2808,  1359,  1113,\n          1184,   117,  1897,  1190,  5228,   136,   102,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327,  5376,  1112,   170,  2952, 20382,  1106,  2848,   118,\n          2163,  6465,   136,   102,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327, 20343,  1108,  9142,  9308,  1111,  1535,   136,   102,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  2102,  1741,  5027, 18025,  1107,  1184,  5027,   136,   102,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327,  1108,  1103, 11242,  1104, 25702, 14410, 24648,  6763,\n           136,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1731,  1263,  1108,  1103, 27153,  1104, 23295,  1107,  1329,\n          1111,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327,  1168,  2505,  2523,  1225,   140, 17312,  6582,  7221,\n          2904,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  9854, 20164, 15114,   112,   188,  6004,  1164, 26341,  2148,\n          1111,  1134,  2749,  1118, 10792,  3180,  1918,  2875,  1874,   136,\n           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327,  1110,  1103,  1248,  1211,  1887,  7309,  1372,  1107,\n         23761,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327,  1250,  1225,  1103,  3167,  5996,  1111,  1335, 17013,\n         20478,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  3982,  1103,  1503,  3884,  1104,  1103,  1148,  2596,  8012,\n          6664,   136,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1130, 12681,  1150,  1225,  3414, 23870,  1116,  3326,  1114,\n          1103,  1494,  1104,  5586, 16588,   136,   102,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  1327, 10548,  1108,  4379,  1112,  1196,  5250,  6610,  7490,\n          2941,   172, 25669,  2155, 13622, 22273,  1108,  2751,   136,   102,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0],\n        [  101,  2777,  1110,  1103,  2683,  2859,  2334,  3782,   136,   102,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}"},"metadata":{}}]},{"cell_type":"code","source":"def predict(model, questions, contexts,labels, tokenizer, max_len):\n    questions = tokenizer(questions, max_length = max_len, padding='max_length', truncation=True, return_tensors='pt')['input_ids'].to('cuda')\n    contexts = tokenizer(contexts, max_length = max_len, padding='max_length', truncation=True, return_tensors='pt')['input_ids'].to('cuda')\n    with torch.no_grad():\n        output = model(questions, contexts)\n        _, predicted = torch.max(output, 1)\n    print(\"predicted label :\", predicted)\n    print(\"actual label :\", labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:11:56.836560Z","iopub.execute_input":"2024-06-14T04:11:56.836949Z","iopub.status.idle":"2024-06-14T04:11:56.843479Z","shell.execute_reply.started":"2024-06-14T04:11:56.836917Z","shell.execute_reply":"2024-06-14T04:11:56.842497Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"predict(model, dataset['train'][2269:2300]['question'], dataset['train'][2269:2300]['sentence'],dataset['train'][2269:2300]['label'], tokenizer, 128)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:12:00.386555Z","iopub.execute_input":"2024-06-14T04:12:00.386941Z","iopub.status.idle":"2024-06-14T04:12:00.486447Z","shell.execute_reply.started":"2024-06-14T04:12:00.386912Z","shell.execute_reply":"2024-06-14T04:12:00.485447Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"predicted label : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n        1, 1, 0, 0, 1, 1, 0], device='cuda:0')\nactual label : [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset['train'][2269:2300]['label']","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:11:50.478044Z","iopub.execute_input":"2024-06-14T04:11:50.478843Z","iopub.status.idle":"2024-06-14T04:11:50.487607Z","shell.execute_reply.started":"2024-06-14T04:11:50.478803Z","shell.execute_reply":"2024-06-14T04:11:50.486447Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"[1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0]"},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:16:57.024201Z","iopub.execute_input":"2024-06-14T04:16:57.025097Z","iopub.status.idle":"2024-06-14T04:16:57.030036Z","shell.execute_reply.started":"2024-06-14T04:16:57.025058Z","shell.execute_reply":"2024-06-14T04:16:57.028843Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"predict(model, dataset['train'][2600:(2600+50)]['question'], dataset['train'][2600:2650]['sentence'],dataset['train'][2600:2650]['label'], tokenizer, 128)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T04:17:20.522353Z","iopub.execute_input":"2024-06-14T04:17:20.522956Z","iopub.status.idle":"2024-06-14T04:17:20.744202Z","shell.execute_reply.started":"2024-06-14T04:17:20.522922Z","shell.execute_reply":"2024-06-14T04:17:20.743010Z"},"trusted":true},"execution_count":52,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2600\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2600\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2600\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2650\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2600\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2650\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[45], line 5\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, questions, contexts, labels, tokenizer, max_len)\u001b[0m\n\u001b[1;32m      3\u001b[0m contexts \u001b[38;5;241m=\u001b[39m tokenizer(contexts, max_length \u001b[38;5;241m=\u001b[39m max_len, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted label :\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mBiDAF.forward\u001b[0;34m(self, q, c)\u001b[0m\n\u001b[1;32m     11\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(q)\n\u001b[1;32m     12\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macontext(c)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mE2E.forward\u001b[0;34m(self, c, q)\u001b[0m\n\u001b[1;32m     39\u001b[0m _q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, c_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m cq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(_c,_q)\n\u001b[0;32m---> 41\u001b[0m input_s \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [bs, clen, qlen, hidden_size*6]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWs(input_s)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#similarity matrix [bs, clen, qlen] \u001b[39;00m\n\u001b[1;32m     45\u001b[0m s1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(s, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.69 GiB. GPU 0 has a total capacty of 15.89 GiB of which 598.12 MiB is free. Process 2110 has 15.31 GiB memory in use. Of the allocated memory 14.72 GiB is allocated by PyTorch, and 283.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 4.69 GiB. GPU 0 has a total capacty of 15.89 GiB of which 598.12 MiB is free. Process 2110 has 15.31 GiB memory in use. Of the allocated memory 14.72 GiB is allocated by PyTorch, and 283.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}