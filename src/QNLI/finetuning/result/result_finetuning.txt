Before fine-tuning: Accuracy: 0.50 batch_size 128 without toke_type_ids
Before fine-tuning: Accuracy: 0.50 batch_size 128 with toke_type_ids

'normal' bs 32, 4 epochs
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8) with token_type_ids
linear schedule with warmup = 0, 
======== Epoch 1 / 4 ========
  Batch 40  of  313.    Elapsed: , Loss 0.7039295100584263
  Batch 80  of  313.    Elapsed: , Loss 0.7022278389812987
  Batch 120  of  313.    Elapsed: , Loss 0.702039558040209
  Batch 160  of  313.    Elapsed: , Loss 0.7021803067337652
  Batch 200  of  313.    Elapsed: , Loss 0.7013014724005514
  Batch 240  of  313.    Elapsed: , Loss 0.7010422776843502
  Batch 280  of  313.    Elapsed: , Loss 0.7005109059428829
Average training loss: 0.70
======== Epoch 2 / 4 ========
  Batch 40  of  313.    Elapsed: , Loss 0.6771163242619213
  Batch 80  of  313.    Elapsed: , Loss 0.6888581102277026
  Batch 120  of  313.    Elapsed: , Loss 0.6921238465742632
  Batch 160  of  313.    Elapsed: , Loss 0.6923038093199642
  Batch 200  of  313.    Elapsed: , Loss 0.6934053212849062
  Batch 240  of  313.    Elapsed: , Loss 0.6945086084460816
  Batch 280  of  313.    Elapsed: , Loss 0.694875576742179
Average training loss: 0.70
======== Epoch 3 / 4 ========
  Batch 40  of  313.    Elapsed: , Loss 0.6813373376683491
  Batch 80  of  313.    Elapsed: , Loss 0.6887493994500902
  Batch 120  of  313.    Elapsed: , Loss 0.6909061300853068
  Batch 160  of  313.    Elapsed: , Loss 0.6916983653299557
  Batch 200  of  313.    Elapsed: , Loss 0.692659624476931
  Batch 240  of  313.    Elapsed: , Loss 0.6932009456068648
  Batch 280  of  313.    Elapsed: , Loss 0.6940432723306676
Average training loss: 0.70
======== Epoch 4 / 4 ========
  Batch 40  of  313.    Elapsed: , Loss 0.6721005468833737
  Batch 80  of  313.    Elapsed: , Loss 0.6801510541527359
  Batch 120  of  313.    Elapsed: , Loss 0.6836403288131903
  Batch 160  of  313.    Elapsed: , Loss 0.6860998920772386
  Batch 200  of  313.    Elapsed: , Loss 0.6855732327076927
  Batch 240  of  313.    Elapsed: , Loss 0.6862707115802528
  Batch 280  of  313.    Elapsed: , Loss 0.6873067405300208
Average training loss: 0.69

Training Accuracy: 0.56

Valid Accuracy: 0.53

'normal' bs 8 with token_type_ids, 3 epoch
optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-6, betas=(0.9,0.98))
linear schedule with warmup = 0, 
======== Epoch 1 / 3 ========
  Batch 40  of  1,250.    Elapsed: , Loss 0.6730481284420665
  Batch 80  of  1,250.    Elapsed: , Loss 0.6856508048964135
  Batch 120  of  1,250.    Elapsed: , Loss 0.6864356841922792
  Batch 160  of  1,250.    Elapsed: , Loss 0.6898927199914588
  Batch 200  of  1,250.    Elapsed: , Loss 0.6906196501717639
  Batch 240  of  1,250.    Elapsed: , Loss 0.6903259605787602
  Batch 280  of  1,250.    Elapsed: , Loss 0.6897514789129916
  Batch 320  of  1,250.    Elapsed: , Loss 0.690673451743022
  Batch 360  of  1,250.    Elapsed: , Loss 0.6897610501899614
  Batch 400  of  1,250.    Elapsed: , Loss 0.6889316824606232
  Batch 440  of  1,250.    Elapsed: , Loss 0.690110631675677
  Batch 480  of  1,250.    Elapsed: , Loss 0.6906172396487357
  Batch 520  of  1,250.    Elapsed: , Loss 0.6911794220646146
  Batch 560  of  1,250.    Elapsed: , Loss 0.6912997442558275
  Batch 600  of  1,250.    Elapsed: , Loss 0.6916459495334975
  Batch 640  of  1,250.    Elapsed: , Loss 0.6908489564279684
  Batch 680  of  1,250.    Elapsed: , Loss 0.69025651237513
  Batch 720  of  1,250.    Elapsed: , Loss 0.6900758835875872
  Batch 760  of  1,250.    Elapsed: , Loss 0.6900578488971495
  Batch 800  of  1,250.    Elapsed: , Loss 0.6905382978038097
  Batch 840  of  1,250.    Elapsed: , Loss 0.6907884227676709
  Batch 880  of  1,250.    Elapsed: , Loss 0.6906541136688596
  Batch 920  of  1,250.    Elapsed: , Loss 0.6904277332313157
  Batch 960  of  1,250.    Elapsed: , Loss 0.690278251821119
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.6905488440325924
  Batch 1,040  of  1,250.    Elapsed: , Loss 0.6907317487811034
  Batch 1,080  of  1,250.    Elapsed: , Loss 0.6905262386434945
  Batch 1,120  of  1,250.    Elapsed: , Loss 0.6904587046468396
  Batch 1,160  of  1,250.    Elapsed: , Loss 0.6904639738269349
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.6904068532633245
  Batch 1,240  of  1,250.    Elapsed: , Loss 0.6906412055567327
Average training loss: 0.69
======== Epoch 2 / 3 ========
  Batch 40  of  1,250.    Elapsed: , Loss 0.6685565404775666
  Batch 80  of  1,250.    Elapsed: , Loss 0.6831083982079117
  Batch 120  of  1,250.    Elapsed: , Loss 0.6825309858834447
  Batch 160  of  1,250.    Elapsed: , Loss 0.6819122755749626
  Batch 200  of  1,250.    Elapsed: , Loss 0.6806349110840565
  Batch 240  of  1,250.    Elapsed: , Loss 0.6817152287455516
  Batch 280  of  1,250.    Elapsed: , Loss 0.6825102436160702
  Batch 320  of  1,250.    Elapsed: , Loss 0.6823249635666702
  Batch 360  of  1,250.    Elapsed: , Loss 0.6822754706041965
  Batch 400  of  1,250.    Elapsed: , Loss 0.6810720513883671
  Batch 440  of  1,250.    Elapsed: , Loss 0.6822014117186843
  Batch 480  of  1,250.    Elapsed: , Loss 0.6815184201123561
  Batch 520  of  1,250.    Elapsed: , Loss 0.6817834169072977
  Batch 560  of  1,250.    Elapsed: , Loss 0.6827216842170281
  Batch 600  of  1,250.    Elapsed: , Loss 0.683325340466174
  Batch 640  of  1,250.    Elapsed: , Loss 0.6832693110390126
  Batch 680  of  1,250.    Elapsed: , Loss 0.6838953807490513
  Batch 720  of  1,250.    Elapsed: , Loss 0.6839225149187731
  Batch 760  of  1,250.    Elapsed: , Loss 0.6836138426395972
  Batch 800  of  1,250.    Elapsed: , Loss 0.6838637004779669
  Batch 840  of  1,250.    Elapsed: , Loss 0.684379732566271
  Batch 880  of  1,250.    Elapsed: , Loss 0.6849334837359278
  Batch 920  of  1,250.    Elapsed: , Loss 0.6849474086305865
  Batch 960  of  1,250.    Elapsed: , Loss 0.6847909344718807
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.6850936742691132
  Batch 1,040  of  1,250.    Elapsed: , Loss 0.68481188646082
  Batch 1,080  of  1,250.    Elapsed: , Loss 0.6847938543993715
  Batch 1,120  of  1,250.    Elapsed: , Loss 0.6846212507570354
  Batch 1,160  of  1,250.    Elapsed: , Loss 0.6844192937876417
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.684243748130449
  Batch 1,240  of  1,250.    Elapsed: , Loss 0.6844028301242856
Average training loss: 0.69
======== Epoch 3 / 3 ========
  Batch 40  of  1,250.    Elapsed: , Loss 0.6803077706476537
  Batch 80  of  1,250.    Elapsed: , Loss 0.6841508606333792
  Batch 120  of  1,250.    Elapsed: , Loss 0.6840819736157567
  Batch 160  of  1,250.    Elapsed: , Loss 0.6811352116721017
  Batch 200  of  1,250.    Elapsed: , Loss 0.6836735060558984
  Batch 240  of  1,250.    Elapsed: , Loss 0.6817429416901838
  Batch 280  of  1,250.    Elapsed: , Loss 0.6808493179782854
  Batch 320  of  1,250.    Elapsed: , Loss 0.6817819581967648
  Batch 360  of  1,250.    Elapsed: , Loss 0.6808331612074474
  Batch 400  of  1,250.    Elapsed: , Loss 0.68097934877486
  Batch 440  of  1,250.    Elapsed: , Loss 0.6807663843474961
  Batch 480  of  1,250.    Elapsed: , Loss 0.6804566103306728
  Batch 520  of  1,250.    Elapsed: , Loss 0.6804101822929968
  Batch 560  of  1,250.    Elapsed: , Loss 0.6814848710508907
  Batch 600  of  1,250.    Elapsed: , Loss 0.6809093358116023
  Batch 640  of  1,250.    Elapsed: , Loss 0.6821456905832157
  Batch 680  of  1,250.    Elapsed: , Loss 0.6824915284802385
  Batch 720  of  1,250.    Elapsed: , Loss 0.6829323534661291
  Batch 760  of  1,250.    Elapsed: , Loss 0.6825796447978226
  Batch 800  of  1,250.    Elapsed: , Loss 0.6822667489486389
  Batch 840  of  1,250.    Elapsed: , Loss 0.6820329645300875
  Batch 880  of  1,250.    Elapsed: , Loss 0.6814345212718821
  Batch 920  of  1,250.    Elapsed: , Loss 0.6817293009851189
  Batch 960  of  1,250.    Elapsed: , Loss 0.682007009541455
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.6818467161276719
  Batch 1,040  of  1,250.    Elapsed: , Loss 0.6828633709439398
  Batch 1,080  of  1,250.    Elapsed: , Loss 0.6821087793439323
  Batch 1,120  of  1,250.    Elapsed: , Loss 0.6823316865686218
  Batch 1,160  of  1,250.    Elapsed: , Loss 0.682042247821913
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.6817383898187934
  Batch 1,240  of  1,250.    Elapsed: , Loss 0.6817807462021385
Average training loss: 0.68

Training Accuracy: 0.59

Validating Accuracy: 0.54



'normal' bs 8, 1 epochs
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-6, betas=(0.9,0.98))
linear schedule with warmup = 0 
======== Epoch 1 / 1 ========
  Batch 100  of  1,250.    Elapsed: , Loss 0.6844614360592153
  Batch 200  of  1,250.    Elapsed: , Loss 0.6885084898317632
  Batch 300  of  1,250.    Elapsed: , Loss 0.6912350915991191
  Batch 400  of  1,250.    Elapsed: , Loss 0.6929032410172156
  Batch 500  of  1,250.    Elapsed: , Loss 0.6937909294031337
  Batch 600  of  1,250.    Elapsed: , Loss 0.6943171976410014
  Batch 700  of  1,250.    Elapsed: , Loss 0.6951948763980675
  Batch 800  of  1,250.    Elapsed: , Loss 0.6954024713137623
  Batch 900  of  1,250.    Elapsed: , Loss 0.694589881732911
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.695105000809356
  Batch 1,100  of  1,250.    Elapsed: , Loss 0.6951255539565818
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.6953168630997009
Average training loss: 0.70
Training Accuracy: 0.53
Validating Accuracy: 0.53


'normal', bs 8, epochs 1,  
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-6, betas=(0.9,0.98), weight_decay =0.01)
 scheduler = get_linear_schedule_with_warmup(optimizer, 
											num_warmup_steps=int(0.06*total_steps),
											num_training_steps=total_steps)
											
											======== Epoch 1 / 1 ========
  Batch 100  of  1,250.    Elapsed: , Loss 0.6997218093659618
  Batch 200  of  1,250.    Elapsed: , Loss 0.7005211789513108
  Batch 300  of  1,250.    Elapsed: , Loss 0.7011169241710359
  Batch 400  of  1,250.    Elapsed: , Loss 0.6995342370131961
  Batch 500  of  1,250.    Elapsed: , Loss 0.6995426647796364
  Batch 600  of  1,250.    Elapsed: , Loss 0.6984976207257904
  Batch 700  of  1,250.    Elapsed: , Loss 0.6991917171678937
  Batch 800  of  1,250.    Elapsed: , Loss 0.6991108161456576
  Batch 900  of  1,250.    Elapsed: , Loss 0.6988265400258339
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.6990248080078777
  Batch 1,100  of  1,250.    Elapsed: , Loss 0.6992000661093359
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.6989456964819556
Average training loss: 0.70

Training accuracy : 0.50
Validating accuracy : 0.52


'normal'. bs 8, epoch 1
optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-6, betas=(0.9,0.98), weight_decay =0.01)

scheduler = get_linear_schedule_with_warmup(optimizer, 
											num_warmup_steps=int(0.06*total_steps),
											num_training_steps=total_steps)
											
======== Epoch 1 / 1 ========
  Batch 100  of  1,250.    Elapsed: , Loss 0.6945292448053265
  Batch 200  of  1,250.    Elapsed: , Loss 0.6986740332337754
  Batch 300  of  1,250.    Elapsed: , Loss 0.6992948861217182
  Batch 400  of  1,250.    Elapsed: , Loss 0.6980719242309988
  Batch 500  of  1,250.    Elapsed: , Loss 0.6978966585414377
  Batch 600  of  1,250.    Elapsed: , Loss 0.6971753705757827
  Batch 700  of  1,250.    Elapsed: , Loss 0.6962285369166974
  Batch 800  of  1,250.    Elapsed: , Loss 0.6961389110329446
  Batch 900  of  1,250.    Elapsed: , Loss 0.6958180727492956
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.6962063680519234
  Batch 1,100  of  1,250.    Elapsed: , Loss 0.6961888913454304
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.6963240431806229
Average training loss: 0.70

Training accuracy 0.56

Validating accuracy 0.54

From the experiment we can see that with lr = 5e-5 we have the greatest performance
then we decide to train the model with lr = 5e-5 with more epoch

'normal', bs = 8, epoch 5
optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-6, betas=(0.9,0.98), weight_decay =0.01)
epochs = 6

scheduler = get_linear_schedule_with_warmup(optimizer, 
											num_warmup_steps=int(0.06*total_steps),
											num_training_steps=total_steps)
											
======== Epoch 1 / 5 ========
  Batch 500  of  1,250.    Elapsed: , Loss 0.6999193562005095
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.6979253273624759
Average training loss: 0.70
======== Epoch 2 / 5 ========
  Batch 500  of  1,250.    Elapsed: , Loss 0.6845499236664611
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.6824219948821492
Average training loss: 0.68
======== Epoch 3 / 5 ========
  Batch 500  of  1,250.    Elapsed: , Loss 0.6594703835760524
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.6568319386356956
Average training loss: 0.66
======== Epoch 4 / 5 ========
  Batch 500  of  1,250.    Elapsed: , Loss 0.6280866612098412
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.6271484943179341
Average training loss: 0.63
======== Epoch 5 / 5 ========
  Batch 500  of  1,250.    Elapsed: , Loss 0.6133158723870199
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.6161643871358343
Average training loss: 0.62

Training Accuracy: 0.70
Validating Accuracy: 0.58

The result above la ket qua khi only fine tuning embedding and classifier
---------------
Thong qua thuc nghiem ta thay fine tune voi lr = 5e-5 co ket qua tot nhat, nen ta se su dung ket qua do fine tuning full weight model

bs 8, epochs 1, 
optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-6, betas=(0.9,0.98), weight_decay =0.01)
scheduler = get_linear_schedule_with_warmup(optimizer, 											num_warmup_steps=int(0.06*total_steps),
											num_training_steps=total_steps)
======== Epoch 1 / 1 ========
  Batch 500  of  1,250.    Elapsed: , Loss 0.5945181456095207
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.5449242926426939
Average training loss: 0.53
Training Accuracy: 0.88
Valid Accuracy: Accuracy: 0.82

batch_size 8, optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-6, betas=(0.9,0.98), weight_decay =0.01)
epochs 2, scheduler = get_linear_schedule_with_warmup(optimizer, 
											num_warmup_steps=int(0.06*total_steps),
											num_training_steps=total_steps)
Fine tuning entire model parameters
======== Epoch 1 / 2 ========
  Batch 200  of  1,250.    Elapsed: , Loss 0.6794830278970709
  Batch 400  of  1,250.    Elapsed: , Loss 0.6444760851766403
  Batch 600  of  1,250.    Elapsed: , Loss 0.6127527945995728
  Batch 800  of  1,250.    Elapsed: , Loss 0.5927456688977657
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.5802010090707185
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.568572939589905
Average training loss: 0.57
======== Epoch 2 / 2 ========
  Batch 200  of  1,250.    Elapsed: , Loss 0.40790664258213777
  Batch 400  of  1,250.    Elapsed: , Loss 0.4046985257203294
  Batch 600  of  1,250.    Elapsed: , Loss 0.3956586726245388
  Batch 800  of  1,250.    Elapsed: , Loss 0.39912180267692954
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.39573572378988925
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.3931263323848467
Average training loss: 0.39
Training Accuracy: 0.92
Validating Accuracy: 0.83

freeze_topk(model, 10) freeze first 10 last mini layer of model 
batch_size 8, optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-6, betas=(0.9,0.98), weight_decay =0.01)
epochs 2, scheduler = get_linear_schedule_with_warmup(optimizer, 
											num_warmup_steps=int(0.06*total_steps),
											num_training_steps=total_steps)
( Models was trained and using the hyperparameter like above, because after fine-tuning with many different hyperparameter we saw that with 
set of hyperparameter like above give the model with greatest result so far and it was used in Prompt-FT in NeurIPS 2023 )
Training Accuracy: 0.96
Validating Accuracy: 0.88
Testing Accuracy: 0.85


Now we experiment with set of new hyperparameter, which was mentioned in ICLR paper in 2021, this paper said that, doi voi bo du lieu nho
ta van co the dat duoc ket qua on dinh, bang cach huan luyen model voi bo du lieu nho do' nhung voi' number of iteration the same as the 
number of iteration, which was used to train model with large dataset

Chung' toi cung~ thu nghiem. fine tuning khi ma` freeze_topk layer in Language Model, as mentioned in ICLR paper in 2021, they prove
that when freeze top k with k >= 10, model dat. duoc ket qua fine-tuning on? dinh. hon, giam di van de catastrophic forgeting, va dong thoi
ket qua sau khi fine tune cung tot hon 

5 epochs, batch_size 8, 
scheduler = get_linear_schedule_with_warmup(optimizer, 
											num_warmup_steps=int(0.06*total_steps),
											num_training_steps=total_steps) 
Freeze last 5 layers in BERT base model 
optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-6, betas=(0.9,0.98), weight_decay =0.01)
======== Epoch 1 / 5 ========
  Batch 200  of  1,250.    Elapsed: , Loss 0.6724465292188065
  Batch 400  of  1,250.    Elapsed: , Loss 0.6132713031776231
  Batch 600  of  1,250.    Elapsed: , Loss 0.5910295805399509
  Batch 800  of  1,250.    Elapsed: , Loss 0.5870405770977785
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.5763602493317811
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.5686889881769088
Average training loss: 0.57
======== Epoch 2 / 5 ========
  Batch 200  of  1,250.    Elapsed: , Loss 0.4239250645782817
  Batch 400  of  1,250.    Elapsed: , Loss 0.4292393242499032
  Batch 600  of  1,250.    Elapsed: , Loss 0.41617783266561303
  Batch 800  of  1,250.    Elapsed: , Loss 0.4237905236079675
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.42418264961251606
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.4209636724883571
Average training loss: 0.42
======== Epoch 3 / 5 ========
  Batch 200  of  1,250.    Elapsed: , Loss 0.30555825335070935
  Batch 400  of  1,250.    Elapsed: , Loss 0.30455695179075076
  Batch 600  of  1,250.    Elapsed: , Loss 0.3119266515221143
  Batch 800  of  1,250.    Elapsed: , Loss 0.30291288450724224
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.3102693808012429
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.30949833801386106
Average training loss: 0.31
======== Epoch 4 / 5 ========
  Batch 200  of  1,250.    Elapsed: , Loss 0.2119071599077526
  Batch 400  of  1,250.    Elapsed: , Loss 0.1917147939261792
  Batch 600  of  1,250.    Elapsed: , Loss 0.19988360325573398
  Batch 800  of  1,250.    Elapsed: , Loss 0.18226432208464213
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.1826667600256911
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.180455049429109
Average training loss: 0.18
======== Epoch 5 / 5 ========
  Batch 200  of  1,250.    Elapsed: , Loss 0.11838233818483089
  Batch 400  of  1,250.    Elapsed: , Loss 0.10289835108431321
  Batch 600  of  1,250.    Elapsed: , Loss 0.09556299728552924
  Batch 800  of  1,250.    Elapsed: , Loss 0.09774925658356087
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.09269212046393903
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.09441808788016667
Average training loss: 0.09
Training Accuracy: 0.99
Validating Accuracy: 0.84
Testing Accuracy: 0.83
all of the result above with seed 264

The following result apply seed = 25, as in the paper ICLR 2021
optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-6, betas=(0.9,0.999), weight_decay =0.01)
epochs = 3, batch_size 16, freeze 5 last layers
scheduler = get_linear_schedule_with_warmup(optimizer, 
											num_warmup_steps=int(0.1*total_steps),
											num_training_steps=total_steps)
BERT base
======== Epoch 1 / 3 ========
  Batch 200  of  1,250.    Elapsed: , Loss 0.6891272248913399
  Batch 400  of  1,250.    Elapsed: , Loss 0.639042005370234
  Batch 600  of  1,250.    Elapsed: , Loss 0.6149237002563754
  Batch 800  of  1,250.    Elapsed: , Loss 0.593522640547428
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.5794182022611102
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.5680136028599679
Average training loss: 0.57
======== Epoch 2 / 3 ========
  Batch 200  of  1,250.    Elapsed: , Loss 0.4303445188115485
  Batch 400  of  1,250.    Elapsed: , Loss 0.4150737450362887
  Batch 600  of  1,250.    Elapsed: , Loss 0.4139629527709498
  Batch 800  of  1,250.    Elapsed: , Loss 0.4091788682454125
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.411313905956579
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.4087503541254754
Average training loss: 0.41
======== Epoch 3 / 3 ========
  Batch 200  of  1,250.    Elapsed: , Loss 0.2593891819480306
  Batch 400  of  1,250.    Elapsed: , Loss 0.25713359350001813
  Batch 600  of  1,250.    Elapsed: , Loss 0.26869966063963036
  Batch 800  of  1,250.    Elapsed: , Loss 0.26925380857220704
  Batch 1,000  of  1,250.    Elapsed: , Loss 0.2688039754618324
  Batch 1,200  of  1,250.    Elapsed: , Loss 0.2672313464001724
Average training loss: 0.27
Training Accuracy: 0.97 
Validating Accuracy: 0.82

We decided to not show results for BERTBASE since previous works observed no instability when 
fine-tuning BERTBASE which we also confirmed in our experiments. cite ICLR 2021 ( instability fine-tuning)


The following result apply seed = 25, as in the paper ICLR 2021
optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-6, betas=(0.9,0.999), weight_decay =0.01)
epochs = 3, batch_size 16, freeze 12 last layers, seed = 25
scheduler = get_linear_schedule_with_warmup(optimizer, 
											num_warmup_steps=int(0.1*total_steps),
											num_training_steps=total_steps)
BERT large
======== Epoch 1 / 3 ========
  Batch 200  of  625.    Elapsed: , Loss 0.694170554330693
  Batch 400  of  625.    Elapsed: , Loss 0.7038691764014617
  Batch 600  of  625.    Elapsed: , Loss 0.7060927048994975
Average training loss: 0.71
======== Epoch 2 / 3 ========
  Batch 200  of  625.    Elapsed: , Loss 0.7054342755037754
  Batch 400  of  625.    Elapsed: , Loss 0.7058024199823489
  Batch 600  of  625.    Elapsed: , Loss 0.7066764218636639
Average training loss: 0.71
======== Epoch 3 / 3 ========
  Batch 200  of  625.    Elapsed: , Loss 0.7072248233491508
  Batch 400  of  625.    Elapsed: , Loss 0.7052844831176529
  Batch 600  of  625.    Elapsed: , Loss 0.7053600155970022
Average training loss: 0.71
Training Accuracy: 0.5
Validating Accuracy : 0.5

The following result apply seed = 25, as in the paper ICLR 2021
optimizer = Adam(model.parameters(), lr=5e-5, eps=1e-6, betas=(0.9,0.999), weight_decay =0.01)
epochs = 3, batch_size 16, freeze 12 last layers, seed = 25
scheduler = get_linear_schedule_with_warmup(optimizer, 
											num_warmup_steps=int(0.1*total_steps),
											num_training_steps=total_steps)

======== Epoch 1 / 3 ========
  Batch 200  of  625.    Elapsed: , Loss 0.6777979249681406
  Batch 400  of  625.    Elapsed: , Loss 0.6087130194070036
  Batch 600  of  625.    Elapsed: , Loss 0.5907140027564298
Average training loss: 0.59
======== Epoch 2 / 3 ========
  Batch 200  of  625.    Elapsed: , Loss 0.6115856788199933
  Batch 400  of  625.    Elapsed: , Loss 0.6432689798071497
  Batch 600  of  625.    Elapsed: , Loss 0.6616876135005332
Average training loss: 0.66
======== Epoch 3 / 3 ========
  Batch 200  of  625.    Elapsed: , Loss 0.6960350500410469
  Batch 400  of  625.    Elapsed: , Loss 0.6973033794143848
  Batch 600  of  625.    Elapsed: , Loss 0.6966849766832818
Average training loss: 0.70
Training Accuracy: 0.5
Validating Accuracy: 0.5


optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-6, betas=(0.9,0.999), weight_decay =0.01)
epochs = 10
scheduler = get_linear_schedule_with_warmup(optimizer, 
											num_warmup_steps=int(0.1*total_steps),
											num_training_steps=total_steps)
Freeze top 12 layers
Training time around 2h or more on GPU P100
======== Epoch 1 / 10 ========
  Batch 200  of  625.    Elapsed: , Loss 0.7026402045838276
  Batch 400  of  625.    Elapsed: , Loss 0.6598273491175692
  Batch 600  of  625.    Elapsed: , Loss 0.5986422675471139
Average training loss: 0.59
======== Epoch 2 / 10 ========
  Batch 200  of  625.    Elapsed: , Loss 0.327097032089435
  Batch 400  of  625.    Elapsed: , Loss 0.3325200519917017
  Batch 600  of  625.    Elapsed: , Loss 0.3374570842689762
Average training loss: 0.34
======== Epoch 3 / 10 ========
  Batch 200  of  625.    Elapsed: , Loss 0.1969907523332099
  Batch 400  of  625.    Elapsed: , Loss 0.188030562874515
  Batch 600  of  625.    Elapsed: , Loss 0.186834754520708
Average training loss: 0.19
======== Epoch 4 / 10 ========
  Batch 200  of  625.    Elapsed: , Loss 0.1085443762802885
  Batch 400  of  625.    Elapsed: , Loss 0.10963612702324466
  Batch 600  of  625.    Elapsed: , Loss 0.11408690571127933
Average training loss: 0.11
======== Epoch 5 / 10 ========
  Batch 200  of  625.    Elapsed: , Loss 0.05675562507581818
  Batch 400  of  625.    Elapsed: , Loss 0.07381995066656659
  Batch 600  of  625.    Elapsed: , Loss 0.07231505961988753
Average training loss: 0.07
======== Epoch 6 / 10 ========
  Batch 200  of  625.    Elapsed: , Loss 0.05895944916879508
  Batch 400  of  625.    Elapsed: , Loss 0.05349607036743392
  Batch 600  of  625.    Elapsed: , Loss 0.060476846549680885
Average training loss: 0.06
======== Epoch 7 / 10 ========
  Batch 200  of  625.    Elapsed: , Loss 0.023446232335588697
  Batch 400  of  625.    Elapsed: , Loss 0.033823041967638856
  Batch 600  of  625.    Elapsed: , Loss 0.033462490958652495
Average training loss: 0.03
======== Epoch 8 / 10 ========
  Batch 200  of  625.    Elapsed: , Loss 0.02227061516233918
  Batch 400  of  625.    Elapsed: , Loss 0.025118846714814794
  Batch 600  of  625.    Elapsed: , Loss 0.027865833587107324
Average training loss: 0.03
======== Epoch 9 / 10 ========
  Batch 200  of  625.    Elapsed: , Loss 0.020543820426583034
  Batch 400  of  625.    Elapsed: , Loss 0.01593052550976933
  Batch 600  of  625.    Elapsed: , Loss 0.0178852494519226
Average training loss: 0.02
======== Epoch 10 / 10 ========
  Batch 200  of  625.    Elapsed: , Loss 0.01462841626549625
  Batch 400  of  625.    Elapsed: , Loss 0.015006505622458963
  Batch 600  of  625.    Elapsed: , Loss 0.014017099442552264
Average training loss: 0.02

Training Accuracy: 1.00 ( actually 0.9992)
Validating Accuracy: 0.88 ( 0.884765625 )
Testing Accuracy: 0.89 ( 0.892578125 )